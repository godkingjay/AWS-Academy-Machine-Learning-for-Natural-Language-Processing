{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 02.14.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6.2: Implementing Topic Extraction with NTM\n",
    "\n",
    "In this lab, you will use the Amazon SageMaker Neural Topic Model (NTM) algorithm to extract topics from the [20 Newsgroups](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups) dataset.\n",
    "\n",
    "The Amazon SageMaker Neural Topic Model (NTM) is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. NTM is most commonly used to discover a user-specified number of topics that are shared by documents within a text corpus.\n",
    "\n",
    "Each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Because the method is unsupervised, the topics are not specified up front and are not guaranteed to align with how a human might naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics. For more information, see [Neural Topic Model (NTM) Algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html) in the Amazon SageMaker Developer Guide.\n",
    "\n",
    "\n",
    "## About this dataset\n",
    "\n",
    "The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. This collection has become a popular dataset for experiments in text applications of machine learning techniques, such as text classification and text clustering. In this lab, you will see what topics you can learn from this set of documents using the Neural Topic Model (NTM) algorithm.\n",
    "\n",
    "Dataset source: Tom Mitchell. *20 Newsgroups Data*. September 9, 1999. Distributed by UCI KDD Archive. https://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab steps\n",
    "\n",
    "1. [Fetching the dataset](#1.-Fetching-the-dataset)\n",
    "2. [Examining and preprocessing the data](#2.-Examining-and-preprocessing-the-data)\n",
    "3. [Preparing the data for training](#3.-Preparing-the-data-for-training)\n",
    "4. [Training the model](#4.-Training-the-model)\n",
    "5. [Using the model for inference](#5.-Using-the-model-for-inference)\n",
    "6. [Exploring the model](#6.-Exploring-the-model)\n",
    "\n",
    "## Submitting your work\n",
    "\n",
    "1. In the lab console, choose **Submit** to record your progress and when prompted, choose **Yes**.\n",
    "\n",
    "1. If the results don't display after a couple of minutes, return to the top of the lab instructions and choose **Grades**.\n",
    "\n",
    "    **Tip:** You can submit your work multiple times. After you change your work, choose **Submit** again. Your last submission is what will be recorded for this lab.\n",
    "\n",
    "1. To find detailed feedback on your work, choose **Details** followed by **View Submission Report**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetching the dataset\n",
    "([Go to top](#Lab-6.2:-Implementing-Topic-Extraction-with-NTM))\n",
    "\n",
    "First, define the folder to hold the data. Then, clean up the folder, which might contain data from previous experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (23.3.1)\n",
      "Requirement already satisfied: SageMaker in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.199.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (1.33.4)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (1.26.1)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (4.24.4)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (6.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (0.3.1)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (4.19.1)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (3.11.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (1.7.0)\n",
      "Requirement already satisfied: urllib3<1.27 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (1.26.18)\n",
      "Requirement already satisfied: uvicorn==0.22.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (0.22.0)\n",
      "Requirement already satisfied: fastapi==0.95.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (0.95.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (4.66.1)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SageMaker) (5.9.5)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fastapi==0.95.2->SageMaker) (1.10.13)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fastapi==0.95.2->SageMaker) (0.27.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from uvicorn==0.22.0->SageMaker) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from uvicorn==0.22.0->SageMaker) (0.14.0)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->SageMaker) (1.33.4)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->SageMaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->SageMaker) (0.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->SageMaker) (3.17.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.0->SageMaker) (3.1.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from docker->SageMaker) (1.6.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->SageMaker) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->SageMaker) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->SageMaker) (2023.7.22)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-pasta->SageMaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->SageMaker) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->SageMaker) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->SageMaker) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->SageMaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->SageMaker) (2023.3.post1)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->SageMaker) (1.7.6.7)\n",
      "Requirement already satisfied: dill>=0.3.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->SageMaker) (0.3.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->SageMaker) (0.3.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->SageMaker) (0.70.15)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from schema->SageMaker) (21.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi==0.95.2->SageMaker) (4.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.95.2->SageMaker) (4.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->SageMaker) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->SageMaker) (1.1.3)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade SageMaker\n",
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def check_create_dir(dir):\n",
    "    if os.path.exists(dir):  # Clean up existing data folder\n",
    "        shutil.rmtree(dir)\n",
    "    os.mkdir(dir)\n",
    "\n",
    "data_dir = '20_newsgroups'\n",
    "check_create_dir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells, you unpack the dataset and extract a list of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\t\t  rec.autos\t      sci.space\n",
      "comp.graphics\t\t  rec.motorcycles     soc.religion.christian\n",
      "comp.os.ms-windows.misc   rec.sport.baseball  talk.politics.guns\n",
      "comp.sys.ibm.pc.hardware  rec.sport.hockey    talk.politics.mideast\n",
      "comp.sys.mac.hardware\t  sci.crypt\t      talk.politics.misc\n",
      "comp.windows.x\t\t  sci.electronics     talk.religion.misc\n",
      "misc.forsale\t\t  sci.med\n"
     ]
    }
   ],
   "source": [
    "!tar -xzf ../s3/20_newsgroups.tar.gz\n",
    "!ls 20_newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 19997\n"
     ]
    }
   ],
   "source": [
    "folders = [os.path.join(data_dir,f) for f in sorted(os.listdir(data_dir)) if os.path.isdir(os.path.join(data_dir, f))]\n",
    "file_list = [os.path.join(d,f) for d in folders for f in os.listdir(d)]\n",
    "print('Number of documents:', len(file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examining and preprocessing the data\n",
    "([Go to top](#Lab-6.2:-Implementing-Topic-Extraction-with-NTM))\n",
    "    \n",
    "In this section, you will examine the data and perform some standard natural language processing (NLP) data cleaning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remind yourself what the files look like in order to determine the best preprocessing steps to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!howland.reston.ans.net!usc!elroy.jpl.nasa.gov!nntp-server.caltech.edu!andrey\n",
      "From: andrey@cco.caltech.edu (Andre T. Yew)\n",
      "Newsgroups: comp.graphics\n",
      "Subject: Re: 16 million vs 65 thousand colors\n",
      "Date: 3 Apr 1993 19:51:06 GMT\n",
      "Organization: California Institute of Technology, Pasadena\n",
      "Lines: 28\n",
      "Message-ID: <1pkpraINNck9@gap.caltech.edu>\n",
      "References: <1993Mar26.210323.27802@midway.uchicago.edu> <dotzlaw-020493084300@murphy.biochem.umanitoba.ca> <d9hh.733845825@dtek.chalmers.se>\n",
      "NNTP-Posting-Host: punisher.caltech.edu\n",
      "\n",
      "d9hh@dtek.chalmers.se (Henrik Harmsen) writes:\n",
      "\n",
      ">1-4 bits per R/G/B gives horrible machbanding visible in almost any picture.\n",
      "\n",
      ">5 bits per R/G/B (32768, 65000 colors) gives visible machbanding\n",
      "\n",
      ">color-gradient picture has _almost_ no machbanding. This color-resolution is \n",
      "\n",
      ">see some small machbanding on the smooth color-gradient picture, but all in all,\n",
      ">There _ARE_ situiations where you get visible mach-banding even in\n",
      ">a 24 bit card. If\n",
      ">you create a very smooth color gradient of dark-green-white-yellow\n",
      ">or something and turn\n",
      ">up the contrast on the monitor, you will probably see some mach-banding.\n",
      "\n",
      "    While I don't mean to damn Henrik's attempt to be helpful here,\n",
      "he's using a common misconception that should be corrected.\n",
      "\n",
      "    Mach banding will occur for any image.  It is not the color\n",
      "quantization you see when you don't have enough bits.  It is the\n",
      "human eye's response to transitions or edges between intensities.\n",
      "The result is that colors near the transistion look brighter on\n",
      "the brighter side and darker on the darker side.\n",
      "\n",
      "--Andre\n",
      "\n",
      "-- \n",
      "             Andre Yew andrey@cco.caltech.edu (131.215.139.2)\n"
     ]
    }
   ],
   "source": [
    "!cat 20_newsgroups/comp.graphics/37917"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each newsgroup document can have the following sections:\n",
    "- header - Contains the standard newsgroup header information. This should be removed.\n",
    "- quoted text - Text from a previous message, which usually is prefixed with '>' or '|', and sometimes starts with *writes*, *wrote*, *said*, or *says*.\n",
    "- message - Body of the message that you want to extract topics from.\n",
    "- footer - Messages typically end with a signature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the following functions, which you will use to remove the headers, quoted text, and footers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def strip_newsgroup_header(text):\n",
    "    \"\"\"\n",
    "    Given text in \"news\" format, strip the headers, by removing everything\n",
    "    before the first blank line.\n",
    "    \"\"\"\n",
    "    _before, _blankline, after = text.partition('\\n\\n')\n",
    "    return after\n",
    "\n",
    "_QUOTE_RE = re.compile(r'(writes in|writes:|wrote:|says:|said:'\n",
    "                       r'|^In article|^Quoted from|^\\||^>)')\n",
    "\n",
    "\n",
    "def strip_newsgroup_quoting(text):\n",
    "    \"\"\"\n",
    "    Given text in \"news\" format, strip lines beginning with the quote\n",
    "    characters > or |, plus lines that often introduce a quoted section\n",
    "    (for example, because they contain the string 'writes:'.)\n",
    "    \"\"\"\n",
    "    good_lines = [line for line in text.split('\\n')\n",
    "                  if not _QUOTE_RE.search(line)]\n",
    "    return '\\n'.join(good_lines)\n",
    "\n",
    "\n",
    "def strip_newsgroup_footer(text):\n",
    "    \"\"\"\n",
    "    Given text in \"news\" format, attempt to remove a signature block.\n",
    "\n",
    "    As a rough heuristic, we assume that signatures are set apart by either\n",
    "    a blank line or a line made of hyphens, and that it is the last such line\n",
    "    in the file (disregarding blank lines at the end).\n",
    "    \"\"\"\n",
    "    lines = text.strip().split('\\n')\n",
    "    for line_num in range(len(lines) - 1, -1, -1):\n",
    "        line = lines[line_num]\n",
    "        if line.strip().strip('-') == '':\n",
    "            break\n",
    "\n",
    "    if line_num > 0:\n",
    "        return '\\n'.join(lines[:line_num])\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages you need for preprocessing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, remove extra spaces, convert the text to lowercase, and lemmatize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def clean(sent):\n",
    "    # Implement this function\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub('\\s+', ' ', sent)\n",
    "    sent = sent.strip()\n",
    "    sent = re.compile('<.*?>').sub('',sent)\n",
    "    # Remove special characters and digits\n",
    "    sent=re.sub(\"(\\\\d|\\\\W)+\",\" \",sent)\n",
    "    sent=re.sub(\"br\",\"\",sent)\n",
    "    filtered_sentence = []\n",
    "    \n",
    "    for w in word_tokenize(sent):\n",
    "        # You are applying custom filtering here. Feel free to try different things.\n",
    "        # Check if it is not numeric, the length > 2, and it is not in stopwords.\n",
    "        if(not w.isnumeric()) and (len(w)>2) and (w not in stop):  \n",
    "            # Stem and add to filtered list\n",
    "            filtered_sentence.append(lem.lemmatize(w))\n",
    "    final_string = \" \".join(filtered_sentence) # Final string of cleaned words\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read each of the newsgroups messages. Remove the header, quotes, and footers. Then, store the results in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "source_group = []\n",
    "for f in file_list:\n",
    "    with open(f, 'rb') as fin:\n",
    "        content = fin.read().decode('latin1')   \n",
    "        content = strip_newsgroup_header(content)\n",
    "        content = strip_newsgroup_quoting(content)\n",
    "        content = strip_newsgroup_footer(content)\n",
    "        content = clean(content)\n",
    "        # Remove header, quoting, and footer\n",
    "        data.append(content)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the entries in the dataset are now just plain text paragraphs. You need to process them into a data format that the NTM algorithm can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['right pretty impossible right would ideal could believe try religion determine course thought suit best possible religion warrant belief belief carry psychological benefit also psychological burden well psychological benefit gained racism could fully understood judged person actually believing racism course parallel happens poor one originated',\n",
       " 'gregg provided even title article support contention also reason believe anti islamic slander job apart prejudice yes mere report time stating bcci followed islamic banking rule gregg know islam good know bcci bad therefore bcci islamic anyone say otherwise obviously spreading slanderous propaganda see someone want provide reference article agree also respond reference article agree mmm yes would intellectually stimulating debate doubtless spend time soc culture islam got special place kill file right next bobby want join post become convinced simply waste time try reason moslem hoping achieve',\n",
       " 'wpr atlanta com bill rawlins newsgroups alt atheism organization dgsid atlanta problem scientist exclude possibility supernatural question origin fair premise utterly reject hypothesis science highest form truth better crap creationists put far able manage distortion half truth taking quote context called human like creature ape human fancifully reconstructed fragment genetic code shown man realted primate fossil record little detail creationists try ignore good deed justify person god sight atonement jesus needed atone sin say bible would surprised christian followed rule bible pick choose according local bias point god creator look like agree proof know god send info via mail regard bill post would interested seeing']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to vectorize the data so that it is ready for training. You can use `CountVectorizer`, which you have used in previous labs, and limit the vocabulary size to `vocab_size`. \n",
    "\n",
    "Use a maximum document frequency of 95 percent of documents (`max_df=0.95`) and a minimum document frequency of 2 documents (`min_df=2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and counting, this may take a few minutes...\n",
      "vocab size: 2000\n",
      "CPU times: user 1.34 s, sys: 16.1 ms, total: 1.35 s\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vocab_size = 2000\n",
    "print('Tokenizing and counting, this may take a few minutes...')\n",
    "\n",
    "# vectorizer = CountVectorizer(input='content', max_features=vocab_size, max_df=0.95, min_df=2)\n",
    "vectorizer = CountVectorizer(input='content', max_features=vocab_size)\n",
    "vectors = vectorizer.fit_transform(data)\n",
    "vocab_list = vectorizer.get_feature_names_out()\n",
    "\n",
    "print('vocab size:', len(vocab_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, consider removing short documents. A short document is not likely to express more than one topic. Topic modeling tries to model each document as a mixture of multiple topics; therefore, topic modeling might not be suitable for short documents.\n",
    "\n",
    "The following cell removes documents that contain fewer than 25 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed short docs (<25 words)\n",
      "(10870, 2000)\n"
     ]
    }
   ],
   "source": [
    "threshold = 25\n",
    "vectors = vectors[np.array(vectors.sum(axis=1)>threshold).reshape(-1,)]\n",
    "print('removed short docs (<{} words)'.format(threshold))        \n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from `CountVectorizer` are sparse matrices with their elements being integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'> int64\n",
      "  (0, 1798)\t1\n",
      "  (0, 1821)\t1\n",
      "  (0, 1438)\t1\n",
      "  (0, 1797)\t1\n",
      "  (0, 952)\t2\n",
      "  (0, 1814)\t4\n",
      "  (0, 84)\t1\n",
      "  (0, 974)\t1\n",
      "  (0, 1657)\t2\n",
      "  (0, 557)\t2\n",
      "  (0, 1856)\t1\n",
      "  (0, 444)\t1\n",
      "  (0, 1718)\t1\n",
      "  (0, 626)\t1\n",
      "  (0, 1278)\t1\n",
      "  (0, 1660)\t1\n",
      "  (0, 345)\t1\n",
      "  (0, 488)\t1\n",
      "  (0, 396)\t1\n",
      "  (0, 1332)\t1\n",
      "  (0, 1859)\t1\n",
      "  (0, 558)\t1\n",
      "  (0, 117)\t1\n",
      "  (0, 179)\t1\n",
      "  (0, 21)\t1\n",
      "  :\t:\n",
      "  (0, 1272)\t2\n",
      "  (0, 1298)\t1\n",
      "  (0, 788)\t2\n",
      "  (0, 1916)\t1\n",
      "  (0, 1781)\t1\n",
      "  (0, 1946)\t1\n",
      "  (0, 1925)\t1\n",
      "  (0, 145)\t1\n",
      "  (0, 1062)\t1\n",
      "  (0, 652)\t1\n",
      "  (0, 1115)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 737)\t1\n",
      "  (0, 1474)\t1\n",
      "  (0, 962)\t1\n",
      "  (0, 295)\t1\n",
      "  (0, 907)\t1\n",
      "  (0, 1991)\t1\n",
      "  (0, 750)\t1\n",
      "  (0, 176)\t1\n",
      "  (0, 1169)\t1\n",
      "  (0, 1995)\t1\n",
      "  (0, 1699)\t1\n",
      "  (0, 904)\t1\n",
      "  (0, 1405)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(vectors), vectors.dtype)\n",
    "print(vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the parameters (weights and biases) in the NTM model are `np.float32` type. Therefore, you need the input data to also be `np.float32` type. It is better to do this type-casting up front rather than repeatedly casting during mini-batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'> float32\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sparse\n",
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "print(type(vectors), vectors.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing the data for training\n",
    "([Go to top](#Lab-6.2:-Implementing-Topic-Extraction-with-NTM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a common practice in model training, you should have a training set, validation set, and test set. The training set is the set of data that the model is actually being trained on. You care about the model's performance on future, unseen data. Therefore, during training, you periodically calculate scores (or losses) on the validation set to validate the performance of the model on unseen data. By assessing the model's ability to generalize, you can stop training at the optimal point to avoid overtraining.\n",
    "\n",
    "Note that when you only have a training set and no validation set, the NTM model will rely on scores on the training set to perform early stopping, which could result in overtraining. Therefore, you should always supply a validation set to the model.\n",
    "\n",
    "In this lab, you will use 80 percent of the dataset as the training set, and the remaining 20 percent for the validation set and test set. You will use the validation set in training and use the test set to demonstrate model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(df):\n",
    "    train, test_validate = train_test_split(df,\n",
    "                                            test_size=0.2,\n",
    "                                            shuffle=True,\n",
    "                                            random_state=324\n",
    "                                            )\n",
    "    test, validate = train_test_split(test_validate,\n",
    "                                            test_size=0.5,\n",
    "                                            shuffle=True,\n",
    "                                            random_state=324\n",
    "                                            )\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, val_vectors, test_vectors = split_data(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8696, 2000) (1087, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape, val_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the vocabulary file\n",
    "\n",
    "To make use of the auxiliary channel for the vocabulary file, first save the text file with the name **vocab.txt** in the **auxiliary** directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "aux_data_dir = os.path.join(data_dir, 'auxiliary')\n",
    "check_create_dir(aux_data_dir)\n",
    "with open(os.path.join(aux_data_dir, 'vocab.txt'), 'w', encoding='utf-8') as f:\n",
    "    for item in vocab_list:\n",
    "        f.write(item+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDITOR COMMENTS for the following cell:\n",
    "- In the first sentence, are \"recordIO\" and \"protobuf\" two different formats, or are they one format? It's written as if they are one format, but the words link to different URLs. From [this page in the SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#cdf-recordio-format), I believe it should be written \"protobuf recordIO format\", but I don't understand why there are two URLs.\n",
    "- The URL for \"RecordIO\" gives a 404 error. The following may be a replacement URL: https://mxnet.apache.org/versions/1.8.0/api/python/docs/api/mxnet/recordio/index.html#mxnet-recordio.\n",
    "- The cell says \"You will convert...\" but then it looks like the actual conversion occurs several cells later. Consider relocating this cell to be closer to the code cell to which it applies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the data on Amazon S3\n",
    "\n",
    "The NTM algorithm accepts data in the [recordIO - protobuf](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html) format. The SageMaker Python API provides helper functions to convert your data into this format. You will convert the data from NumPy/SciPy and then upload it to an Amazon Simple Storage Service (Amazon S3) destination for the model to access during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDITOR COMMENT for the following cell: The cell references \"boto regexp\", but I don't see that text in any of the code cells within this lab. Should \"boto regexp\" be in one of the code cells? Should \"boto regexp\" be replaced with something else here? Is that note applicable to this lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up AWS credentials\n",
    "\n",
    "You first need to specify data locations and access roles. In particular, you need the following data:\n",
    "\n",
    "- The S3 `bucket` and `prefix` that you want to use for the training and model data. This should be within the same Region as the notebook instance, training, and hosting.\n",
    "- The AWS Identity and Access Management (IAM) `role` is used to give training and hosting access to your data. See the documentation for how to create these. **Note:** If more than one role is required for notebook instances, training, and/or hosting, replace the `boto regexp` with the appropriate full IAM role Amazon Resource Number (ARN) string or strings.\n",
    "\n",
    "**Note:** These values will have been supplied when the lab starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = \"c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set location s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/train\n",
      "Validation set location s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/val\n",
      "Auxiliary data location s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/auxiliary\n",
      "Trained model will be saved at s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/output\n"
     ]
    }
   ],
   "source": [
    "prefix = '20newsgroups-ntm'\n",
    "\n",
    "train_prefix = os.path.join(prefix, 'train')\n",
    "val_prefix = os.path.join(prefix, 'val')\n",
    "aux_prefix = os.path.join(prefix, 'auxiliary')\n",
    "output_prefix = os.path.join(prefix, 'output')\n",
    "\n",
    "s3_train_data = os.path.join('s3://', bucket, train_prefix)\n",
    "s3_val_data = os.path.join('s3://', bucket, val_prefix)\n",
    "s3_aux_data = os.path.join('s3://', bucket, aux_prefix)\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)\n",
    "print('Training set location', s3_train_data)\n",
    "print('Validation set location', s3_val_data)\n",
    "print('Auxiliary data location', s3_aux_data)\n",
    "print('Trained model will be saved at', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define a helper function to convert the data to the recordIO protobuf format and upload it to Amazon S3. In addition, you will have the option to split the data into several parts as specified by `n_parts`.\n",
    "\n",
    "The algorithm inherently supports multiple files in the training folder (\"channel\"), which could be helpful for a large dataset. In addition, when you use distributed training with multiple workers (compute instances), having multiple files enables you to distribute different portions of the training data to different workers.\n",
    "\n",
    "This helper function uses the `write_spmatrix_to_sparse_tensor` function, provided by the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk), to convert SciPy sparse matrix into the recordIO protobuf format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_convert_upload(sparray, bucket, prefix, fname_template='data_part{}.pbr', n_parts=2):\n",
    "    import io\n",
    "    import boto3\n",
    "    import sagemaker.amazon.common as smac\n",
    "    \n",
    "    chunk_size = sparray.shape[0]// n_parts\n",
    "    for i in range(n_parts):\n",
    "\n",
    "        # Calculate start and end indices\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        if i+1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "        \n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # Upload to S3 location specified by bucket and prefix\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "        print('Uploaded data to s3://{}'.format(os.path.join(bucket, fname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data to s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/train/train_part0.pbr\n",
      "Uploaded data to s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/train/train_part1.pbr\n",
      "Uploaded data to s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/train/train_part2.pbr\n",
      "Uploaded data to s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/train/train_part3.pbr\n",
      "Uploaded data to s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/train/train_part4.pbr\n",
      "Uploaded data to s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/train/train_part5.pbr\n",
      "Uploaded data to s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/train/train_part6.pbr\n",
      "Uploaded data to s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/train/train_part7.pbr\n",
      "Uploaded data to s3://c100688a2296028l5426256t1w437248787701-labbucket-zln85o8tvd8u/20newsgroups-ntm/val/val_part0.pbr\n"
     ]
    }
   ],
   "source": [
    "split_convert_upload(train_vectors, bucket=bucket, prefix=train_prefix, fname_template='train_part{}.pbr', n_parts=8)\n",
    "split_convert_upload(val_vectors, bucket=bucket, prefix=val_prefix, fname_template='val_part{}.pbr', n_parts=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the vocab.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.resource('s3').Bucket(bucket).Object(aux_prefix+'/vocab.txt').upload_file(aux_data_dir+'/vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training the model\n",
    "([Go to top](#Lab-6.2:-Implementing-Topic-Extraction-with-NTM))\n",
    "\n",
    "You have created the training and validation datasets and uploaded them to Amazon S3. Next, configure a SageMaker training job to use the NTM algorithm on the data that you prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "container = retrieve('ntm',boto3.Session().region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell automatically chooses an algorithm container based on the current Region. In the API call to `sagemaker.estimator.Estimator`, you also specify the type and count of instances for the training job. Because the 20 Newsgroups dataset is relatively small, you can use a CPU-only instance (`ml.c4.xlarge`).\n",
    "\n",
    "NTM fully takes advantage of GPU hardware and, in general, trains roughly an order of magnitude faster on a GPU than on a CPU. Multi-GPU or multi-instance training further improves training speed roughly linearly if communication overhead is low compared to compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=2, \n",
    "                                    instance_type='ml.c4.xlarge',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sagemaker.Session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDITOR COMMENT for following cell: This appears to be the only subsection within this task. In general, don't use subsections if only one exists. Recommend removing this heading or adding headings to other subsections within this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the hyperparameters\n",
    "\n",
    "The following is a partial list of hyperparameters. For the full list of available hyperparameters, see [NTM Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/ntm_hyperparameters.html) in the Amazon SageMaker Developer Guide.\n",
    "\n",
    "- **feature_dim** - The \"feature dimension\", which should be set to the vocabulary size\n",
    "- **num_topics** - The number of topics to extract\n",
    "- **mini_batch_size** - The batch size for each worker instance. Note that in multi-GPU instances, this number will be further divided by the number of GPUs. For example, if you plan to train on an 8-GPU machine (such as `ml.p2.8xlarge`) and want each GPU to have 1024 training examples per batch, `mini_batch_size` should be set to 8196.\n",
    "- **epochs** - The maximum number of epochs to train for; training may stop early\n",
    "- **num_patience_epochs** and **tolerance** - Control the early stopping behavior. In general, early stopping occurs when there hasn't been improvement on validation loss within the last `num_patience_epochs` number of epochs. Improvements smaller than `tolerance` are considered non-improvement.\n",
    "- **optimizer** and **learning_rate** - The default optimizer is `adadelta`, and `learning_rate` does not need to be set. For other optimizers, the choice of an appropriate learning rate may require experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "ntm.set_hyperparameters(num_topics=num_topics, \n",
    "                        feature_dim=vocab_size, \n",
    "                        mini_batch_size=256, \n",
    "                        num_patience_epochs=10, \n",
    "                        optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify how the training data and validation data will be distributed to the workers during training. Data channels have two modes:\n",
    "\n",
    "- `FullyReplicated`: All data files will be copied to all workers.\n",
    "- `ShardedByS3Key`: Data files will be sharded to different workers. Each worker will receive a different portion of the full dataset.\n",
    "\n",
    "The Python SDK uses the `FullyReplicated` mode for all data channels by default. This is desirable for the validation (test) channel but not for training channel. The reason is that when you use multiple workers, you would like to go through the full dataset by having each worker go through a different portion of the dataset to provide different gradients within epochs. When you use the `FullyReplicated` mode on training data, the training time per epoch is slower (nearly 1.5 times in this example), and it defeats the purpose of distributed training. To set the training data channel correctly, you specify `distribution` to be `ShardedByS3Key` for the training data channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "# sagemaker.inputs.TrainingInput\n",
    "s3_train = TrainingInput(s3_train_data, distribution='ShardedByS3Key') \n",
    "s3_val = TrainingInput(s3_val_data, distribution='FullyReplicated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step before training is to define the auxiliary file. This will replace integers in the log files with the actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_aux = TrainingInput(s3_aux_data, distribution='FullyReplicated', content_type='text/plain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to train. The following cell takes a few minutes to run. The command will first provision the required hardware. You will see a series of dots indicating the progress of the hardware provisioning process. Once the resources are allocated, training logs will be displayed. With multiple workers, the log color and the ID following `INFO` identifies logs that are emitted by different workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: ntm-2023-12-07-05-56-29-770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-07 05:56:30 Starting - Starting the training job...\n",
      "2023-12-07 05:56:45 Starting - Preparing the instances for training.........\n",
      "2023-12-07 05:58:27 Downloading - Downloading input data...\n",
      "2023-12-07 05:58:57 Training - Downloading the training image.....................\n",
      "2023-12-07 06:02:28 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '2000', 'mini_batch_size': '256', 'num_patience_epochs': '10', 'num_topics': '20', 'optimizer': 'adam'}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adam', 'tolerance': '0.001', 'num_patience_epochs': '10', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '2000', 'num_topics': '20'}\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] nvidia-smi: took 0.030 seconds to run.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[35mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[35mRunning default environment configuration script\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '2000', 'mini_batch_size': '256', 'num_patience_epochs': '10', 'num_topics': '20', 'optimizer': 'adam'}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adam', 'tolerance': '0.001', 'num_patience_epochs': '10', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '2000', 'num_topics': '20'}\u001b[0m\n",
      "\u001b[35m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] nvidia-smi: took 0.031 seconds to run.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] Launching parameter server for role server\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-174-76.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2023-12-07-05-56-29-770', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:437248787701:training-job/ntm-2023-12-07-05-56-29-770', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-22bb5f2a6d92ebb5f53797925a27c733392d241bc8a76ebee97ae4736c6913a9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.8/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-174-76.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2023-12-07-05-56-29-770', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:437248787701:training-job/ntm-2023-12-07-05-56-29-770', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-22bb5f2a6d92ebb5f53797925a27c733392d241bc8a76ebee97ae4736c6913a9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.8/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'server', 'DMLC_PS_ROOT_URI': '10.2.145.55', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] Environment: {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-174-76.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2023-12-07-05-56-29-770', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:437248787701:training-job/ntm-2023-12-07-05-56-29-770', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-22bb5f2a6d92ebb5f53797925a27c733392d241bc8a76ebee97ae4736c6913a9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.8/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'worker', 'DMLC_PS_ROOT_URI': '10.2.145.55', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[35mProcess 46 is a shell:server.\u001b[0m\n",
      "\u001b[35mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] Using default worker.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:36.391] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] Initializing\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] vocab.txt\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:36 INFO 140221811656512] Loading pre-trained token embedding vectors from /opt/amazon/lib/python3.8/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-145-55.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2023-12-07-05-56-29-770', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:437248787701:training-job/ntm-2023-12-07-05-56-29-770', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-22bb5f2a6d92ebb5f53797925a27c733392d241bc8a76ebee97ae4736c6913a9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.8/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-145-55.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2023-12-07-05-56-29-770', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:437248787701:training-job/ntm-2023-12-07-05-56-29-770', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-22bb5f2a6d92ebb5f53797925a27c733392d241bc8a76ebee97ae4736c6913a9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.8/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'scheduler', 'DMLC_PS_ROOT_URI': '10.2.145.55', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-145-55.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2023-12-07-05-56-29-770', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:437248787701:training-job/ntm-2023-12-07-05-56-29-770', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-22bb5f2a6d92ebb5f53797925a27c733392d241bc8a76ebee97ae4736c6913a9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.8/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-145-55.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2023-12-07-05-56-29-770', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:437248787701:training-job/ntm-2023-12-07-05-56-29-770', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-22bb5f2a6d92ebb5f53797925a27c733392d241bc8a76ebee97ae4736c6913a9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.8/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'server', 'DMLC_PS_ROOT_URI': '10.2.145.55', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] Environment: {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-145-55.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2023-12-07-05-56-29-770', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:437248787701:training-job/ntm-2023-12-07-05-56-29-770', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-22bb5f2a6d92ebb5f53797925a27c733392d241bc8a76ebee97ae4736c6913a9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.8/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SAGEMAKER_MANAGED_WARMPOOL_CACHE_DIRECTORY': '/opt/ml/sagemaker/warmpoolcache', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'worker', 'DMLC_PS_ROOT_URI': '10.2.145.55', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34mProcess 46 is a shell:scheduler.\u001b[0m\n",
      "\u001b[34mProcess 55 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] Using default worker.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:35.897] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] Initializing\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] vocab.txt\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:35 INFO 140009233762112] Loading pre-trained token embedding vectors from /opt/amazon/lib/python3.8/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:44 WARNING 140009233762112] 13 out of 2000 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:44 INFO 140009233762112] Vocab embedding shape: (2000, 50)\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:44 INFO 140009233762112] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:44 INFO 140009233762112] Create Store: dist_async\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:44 WARNING 140221811656512] 13 out of 2000 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:44 INFO 140221811656512] Vocab embedding shape: (2000, 50)\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:44 INFO 140221811656512] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:44 INFO 140221811656512] Create Store: dist_async\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928965.0985, \"EndTime\": 1701928965.098533, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:45.098] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 8710, \"num_examples\": 1, \"num_bytes\": 106916}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:45.601] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 502, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] # Finished training epoch 1 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] Loss (name: value) total: 7.2450031673207\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] Loss (name: value) kld: 0.01280446387553478\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] Loss (name: value) recons: 7.232198687160716\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] Loss (name: value) logppx: 7.2450031673207\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] #quality_metric: host=algo-2, epoch=1, train total_loss <loss>=7.2450031673207\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:45.606] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 9215, \"num_examples\": 1, \"num_bytes\": 98020}\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:45.651] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 44, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] Loss (name: value) total: 7.154886364936829\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] Loss (name: value) kld: 0.0012268351565580815\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] Loss (name: value) recons: 7.1536595821380615\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] Loss (name: value) logppx: 7.154886364936829\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] #validation_score (1): 7.154886364936829\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] Timing: train: 0.51s, val: 0.05s, epoch: 0.55s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] #progress_metric: host=algo-2, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928965.0989869, \"EndTime\": 1701928965.6536074, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Total Batches Seen\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7837.162330912015 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:45 INFO 140221811656512] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928965.098529, \"EndTime\": 1701928965.0985608, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:45.098] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 9204, \"num_examples\": 1, \"num_bytes\": 88196}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:45.589] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 490, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] # Finished training epoch 1 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] Loss (name: value) total: 7.245679154115565\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] Loss (name: value) kld: 0.012533171903615928\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] Loss (name: value) recons: 7.2331459942985985\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] Loss (name: value) logppx: 7.245679154115565\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=7.245679154115565\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:45.594] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 9696, \"num_examples\": 1, \"num_bytes\": 98020}\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:45.648] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 53, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] Loss (name: value) total: 7.155533671379089\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] Loss (name: value) kld: 0.00116770263412036\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] Loss (name: value) recons: 7.1543660163879395\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] Loss (name: value) logppx: 7.155533671379089\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] #validation_score (1): 7.155533671379089\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] Timing: train: 0.50s, val: 0.06s, epoch: 0.55s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928965.0989168, \"EndTime\": 1701928965.650483, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Total Batches Seen\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7880.973949809704 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:45 INFO 140009233762112] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:46.143] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 489, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] # Finished training epoch 2 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) total: 7.134265030131621\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) kld: 0.0018643939196515608\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) recons: 7.132400568793802\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) logppx: 7.134265030131621\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] #quality_metric: host=algo-2, epoch=2, train total_loss <loss>=7.134265030131621\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:46.211] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 64, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) total: 7.1369030475616455\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) kld: 0.002542035246733576\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) recons: 7.134361028671265\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) logppx: 7.1369030475616455\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] #validation_score (2): 7.1369030475616455\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Timing: train: 0.49s, val: 0.07s, epoch: 0.56s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] #progress_metric: host=algo-2, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928965.6539717, \"EndTime\": 1701928966.2167807, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 8696.0, \"count\": 1, \"min\": 8696, \"max\": 8696}, \"Total Batches Seen\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7723.521822525513 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:46.755] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 538, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] # Finished training epoch 3 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) total: 7.1173984864178825\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) kld: 0.005105143604690538\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) recons: 7.112293243408203\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) logppx: 7.1173984864178825\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] #quality_metric: host=algo-2, epoch=3, train total_loss <loss>=7.1173984864178825\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:46.807] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 49, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) total: 7.139755487442017\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) kld: 0.004366480396129191\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) recons: 7.135388970375061\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Loss (name: value) logppx: 7.139755487442017\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] #validation_score (3): 7.139755487442017\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] Timing: train: 0.54s, val: 0.05s, epoch: 0.59s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] #progress_metric: host=algo-2, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928966.2170556, \"EndTime\": 1701928966.8086889, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 13044.0, \"count\": 1, \"min\": 13044, \"max\": 13044}, \"Total Batches Seen\": {\"sum\": 51.0, \"count\": 1, \"min\": 51, \"max\": 51}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7347.15991106137 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:46 INFO 140221811656512] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:46.085] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 433, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] # Finished training epoch 2 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) total: 7.135462676777559\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) kld: 0.0017858981839655077\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) recons: 7.1336768935708434\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) logppx: 7.135462676777559\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=7.135462676777559\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:46.132] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 44, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) total: 7.139305472373962\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) kld: 0.0021959804580546916\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) recons: 7.137109398841858\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) logppx: 7.139305472373962\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] #validation_score (2): 7.139305472373962\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Timing: train: 0.44s, val: 0.05s, epoch: 0.49s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928965.6507785, \"EndTime\": 1701928966.1381123, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 8696.0, \"count\": 1, \"min\": 8696, \"max\": 8696}, \"Total Batches Seen\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8919.494333377026 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:46.646] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 508, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] # Finished training epoch 3 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) total: 7.119564280790441\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) kld: 0.004738629110814894\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) recons: 7.114825613358441\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) logppx: 7.119564280790441\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=7.119564280790441\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:46.694] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 46, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) total: 7.1346211433410645\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) kld: 0.00964581617154181\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) recons: 7.124975323677063\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Loss (name: value) logppx: 7.1346211433410645\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] #validation_score (3): 7.1346211433410645\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] Timing: train: 0.51s, val: 0.05s, epoch: 0.56s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928966.1383789, \"EndTime\": 1701928966.7018695, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 13044.0, \"count\": 1, \"min\": 13044, \"max\": 13044}, \"Total Batches Seen\": {\"sum\": 51.0, \"count\": 1, \"min\": 51, \"max\": 51}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7713.789774577804 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:46 INFO 140009233762112] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:47.323] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 514, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] # Finished training epoch 4 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) total: 7.126928077024572\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) kld: 0.004081246330101481\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) recons: 7.122846799738267\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) logppx: 7.126928077024572\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] #quality_metric: host=algo-2, epoch=4, train total_loss <loss>=7.126928077024572\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:47.382] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 57, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) total: 7.136035919189453\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) kld: 0.001127462601289153\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) recons: 7.134908437728882\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) logppx: 7.136035919189453\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] #validation_score (4): 7.136035919189453\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Timing: train: 0.52s, val: 0.06s, epoch: 0.58s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] #progress_metric: host=algo-2, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928966.8090377, \"EndTime\": 1701928967.3892553, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 17392.0, \"count\": 1, \"min\": 17392, \"max\": 17392}, \"Total Batches Seen\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7490.828423264211 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:47.172] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 469, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] # Finished training epoch 4 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) total: 7.1284794526941635\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) kld: 0.004976632265264497\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) recons: 7.1235028996187095\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) logppx: 7.1284794526941635\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=7.1284794526941635\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:47.231] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 58, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) total: 7.135199546813965\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) kld: 0.0017657560529187322\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) recons: 7.133433818817139\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) logppx: 7.135199546813965\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] #validation_score (4): 7.135199546813965\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Timing: train: 0.47s, val: 0.06s, epoch: 0.53s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928966.7023563, \"EndTime\": 1701928967.2329102, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 17392.0, \"count\": 1, \"min\": 17392, \"max\": 17392}, \"Total Batches Seen\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8192.136156816803 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:47.713] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 479, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] # Finished training epoch 5 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) total: 7.115950892953312\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) kld: 0.0029206379755016636\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) recons: 7.113030181211584\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) logppx: 7.115950892953312\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=7.115950892953312\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:47.760] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 45, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) total: 7.116005301475525\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) kld: 0.006408361368812621\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) recons: 7.109596848487854\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Loss (name: value) logppx: 7.116005301475525\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] #validation_score (5): 7.116005301475525\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] Timing: train: 0.48s, val: 0.05s, epoch: 0.53s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928967.2332475, \"EndTime\": 1701928967.766046, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 21740.0, \"count\": 1, \"min\": 21740, \"max\": 21740}, \"Total Batches Seen\": {\"sum\": 85.0, \"count\": 1, \"min\": 85, \"max\": 85}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8158.160602305617 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:47 INFO 140009233762112] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:47.889] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 499, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] # Finished training epoch 5 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) total: 7.109957498662612\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) kld: 0.004752607687431223\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) recons: 7.105204834657557\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) logppx: 7.109957498662612\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] #quality_metric: host=algo-2, epoch=5, train total_loss <loss>=7.109957498662612\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:47.955] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 65, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) total: 7.114254474639893\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) kld: 0.006217690068297088\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) recons: 7.108036637306213\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Loss (name: value) logppx: 7.114254474639893\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] #validation_score (5): 7.114254474639893\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] Timing: train: 0.50s, val: 0.07s, epoch: 0.57s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] #progress_metric: host=algo-2, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928967.3896568, \"EndTime\": 1701928967.9607193, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 21740.0, \"count\": 1, \"min\": 21740, \"max\": 21740}, \"Total Batches Seen\": {\"sum\": 85.0, \"count\": 1, \"min\": 85, \"max\": 85}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7611.569671310085 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:47 INFO 140221811656512] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:48.476] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 515, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] # Finished training epoch 6 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] Loss (name: value) total: 7.100986452663646\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] Loss (name: value) kld: 0.00591701951206607\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] Loss (name: value) recons: 7.0950694645152375\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] Loss (name: value) logppx: 7.100986452663646\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] #quality_metric: host=algo-2, epoch=6, train total_loss <loss>=7.100986452663646\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:48.550] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 72, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] Loss (name: value) total: 7.110749006271362\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] Loss (name: value) kld: 0.006056659389287233\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] Loss (name: value) recons: 7.104692220687866\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] Loss (name: value) logppx: 7.110749006271362\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] #validation_score (6): 7.110749006271362\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] Timing: train: 0.52s, val: 0.08s, epoch: 0.59s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] #progress_metric: host=algo-2, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928967.9611046, \"EndTime\": 1701928968.5552597, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 26088.0, \"count\": 1, \"min\": 26088, \"max\": 26088}, \"Total Batches Seen\": {\"sum\": 102.0, \"count\": 1, \"min\": 102, \"max\": 102}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7315.553582295212 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:48 INFO 140221811656512] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:48.232] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 466, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] # Finished training epoch 6 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) total: 7.104499059564927\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) kld: 0.006000457495889243\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) recons: 7.098498624913833\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) logppx: 7.104499059564927\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=7.104499059564927\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:48.282] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 48, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) total: 7.111607313156128\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) kld: 0.005435482249595225\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) recons: 7.10617196559906\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) logppx: 7.111607313156128\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] #validation_score (6): 7.111607313156128\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Timing: train: 0.47s, val: 0.05s, epoch: 0.52s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928967.766371, \"EndTime\": 1701928968.2875743, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 26088.0, \"count\": 1, \"min\": 26088, \"max\": 26088}, \"Total Batches Seen\": {\"sum\": 102.0, \"count\": 1, \"min\": 102, \"max\": 102}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8339.827299018254 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:48.854] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 566, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:49.247] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 687, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] # Finished training epoch 7 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] Loss (name: value) total: 7.099906753091251\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] Loss (name: value) kld: 0.008251177393557393\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] Loss (name: value) recons: 7.09165553485646\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] Loss (name: value) logppx: 7.099906753091251\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] #quality_metric: host=algo-2, epoch=7, train total_loss <loss>=7.099906753091251\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:49.323] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 20, \"duration\": 74, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] Loss (name: value) total: 7.129110813140869\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] Loss (name: value) kld: 0.01063177757896483\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] Loss (name: value) recons: 7.118479132652283\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] Loss (name: value) logppx: 7.129110813140869\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] #validation_score (7): 7.129110813140869\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] Timing: train: 0.69s, val: 0.08s, epoch: 0.77s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] #progress_metric: host=algo-2, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928968.5556567, \"EndTime\": 1701928969.3242908, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 30436.0, \"count\": 1, \"min\": 30436, \"max\": 30436}, \"Total Batches Seen\": {\"sum\": 119.0, \"count\": 1, \"min\": 119, \"max\": 119}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=5655.609062578208 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:49 INFO 140221811656512] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] # Finished training epoch 7 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) total: 7.099507387946634\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) kld: 0.006763638791573399\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) recons: 7.092743733349969\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) logppx: 7.099507387946634\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=7.099507387946634\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:48.898] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 20, \"duration\": 42, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) total: 7.1070709228515625\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) kld: 0.007169182179495692\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) recons: 7.099901914596558\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Loss (name: value) logppx: 7.1070709228515625\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] #validation_score (7): 7.1070709228515625\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] Timing: train: 0.57s, val: 0.05s, epoch: 0.62s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928968.2879746, \"EndTime\": 1701928968.9041464, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 30436.0, \"count\": 1, \"min\": 30436, \"max\": 30436}, \"Total Batches Seen\": {\"sum\": 119.0, \"count\": 1, \"min\": 119, \"max\": 119}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7054.788860008921 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:48 INFO 140009233762112] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:49.377] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 472, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] # Finished training epoch 8 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) total: 7.113455323611989\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) kld: 0.011076984461396933\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) recons: 7.102378284229951\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) logppx: 7.113455323611989\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=7.113455323611989\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:49.427] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 23, \"duration\": 49, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) total: 7.1419419050216675\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) kld: 0.012978735379874706\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) recons: 7.128963112831116\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) logppx: 7.1419419050216675\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] #validation_score (8): 7.1419419050216675\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Timing: train: 0.47s, val: 0.05s, epoch: 0.52s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928968.9044647, \"EndTime\": 1701928969.429205, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 34784.0, \"count\": 1, \"min\": 34784, \"max\": 34784}, \"Total Batches Seen\": {\"sum\": 136.0, \"count\": 1, \"min\": 136, \"max\": 136}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8283.471918742474 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:49.871] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 442, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:50.031] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 706, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] # Finished training epoch 8 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) total: 7.108750567716711\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) kld: 0.007362642996561001\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) recons: 7.101387865403119\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) logppx: 7.108750567716711\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] #quality_metric: host=algo-2, epoch=8, train total_loss <loss>=7.108750567716711\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:50.106] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 23, \"duration\": 73, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) total: 7.112606167793274\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) kld: 0.011219554813578725\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) recons: 7.101386547088623\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) logppx: 7.112606167793274\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] #validation_score (8): 7.112606167793274\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Timing: train: 0.71s, val: 0.08s, epoch: 0.79s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] #progress_metric: host=algo-2, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928969.324571, \"EndTime\": 1701928970.1105354, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 34784.0, \"count\": 1, \"min\": 34784, \"max\": 34784}, \"Total Batches Seen\": {\"sum\": 136.0, \"count\": 1, \"min\": 136, \"max\": 136}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=5528.258212055817 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] # Finished training epoch 9 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) total: 7.106646902420941\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) kld: 0.006143356495372513\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) recons: 7.100503612967098\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) logppx: 7.106646902420941\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=7.106646902420941\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:49.917] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 26, \"duration\": 44, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) total: 7.1096824407577515\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) kld: 0.00510265352204442\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) recons: 7.10457980632782\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Loss (name: value) logppx: 7.1096824407577515\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] #validation_score (9): 7.1096824407577515\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] Timing: train: 0.44s, val: 0.05s, epoch: 0.49s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928969.4294982, \"EndTime\": 1701928969.9193609, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 39132.0, \"count\": 1, \"min\": 39132, \"max\": 39132}, \"Total Batches Seen\": {\"sum\": 153.0, \"count\": 1, \"min\": 153, \"max\": 153}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8873.590108121864 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:49 INFO 140009233762112] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:50.483] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 563, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] # Finished training epoch 10 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] Loss (name: value) total: 7.094403407152961\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] Loss (name: value) kld: 0.008734772447496653\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] Loss (name: value) recons: 7.085668563842773\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] Loss (name: value) logppx: 7.094403407152961\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=7.094403407152961\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:50.560] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 29, \"duration\": 75, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] Loss (name: value) total: 7.102047443389893\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] Loss (name: value) kld: 0.01088091591373086\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] Loss (name: value) recons: 7.0911664962768555\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] Loss (name: value) logppx: 7.102047443389893\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] #validation_score (10): 7.102047443389893\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] Timing: train: 0.56s, val: 0.08s, epoch: 0.65s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928969.9196477, \"EndTime\": 1701928970.5661027, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 43480.0, \"count\": 1, \"min\": 43480, \"max\": 43480}, \"Total Batches Seen\": {\"sum\": 170.0, \"count\": 1, \"min\": 170, \"max\": 170}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=6724.389617716239 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:50 INFO 140009233762112] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:50.873] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 761, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] # Finished training epoch 9 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) total: 7.096522583680994\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) kld: 0.0087009647532421\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) recons: 7.087821567759795\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) logppx: 7.096522583680994\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] #quality_metric: host=algo-2, epoch=9, train total_loss <loss>=7.096522583680994\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:50.951] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 26, \"duration\": 76, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) total: 7.096985578536987\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) kld: 0.009804290486499667\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) recons: 7.087181329727173\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Loss (name: value) logppx: 7.096985578536987\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] #validation_score (9): 7.096985578536987\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] Timing: train: 0.76s, val: 0.08s, epoch: 0.85s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] #progress_metric: host=algo-2, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928970.1114187, \"EndTime\": 1701928970.9587376, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 39132.0, \"count\": 1, \"min\": 39132, \"max\": 39132}, \"Total Batches Seen\": {\"sum\": 153.0, \"count\": 1, \"min\": 153, \"max\": 153}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=5130.120298014968 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:50 INFO 140221811656512] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:51.708] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 745, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] # Finished training epoch 10 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] Loss (name: value) total: 7.082390027887681\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] Loss (name: value) kld: 0.012544674908413607\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] Loss (name: value) recons: 7.069845423978918\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] Loss (name: value) logppx: 7.082390027887681\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] #quality_metric: host=algo-2, epoch=10, train total_loss <loss>=7.082390027887681\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:51.763] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 29, \"duration\": 53, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] Loss (name: value) total: 7.090169429779053\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] Loss (name: value) kld: 0.011633238987997174\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] Loss (name: value) recons: 7.078536152839661\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] Loss (name: value) logppx: 7.090169429779053\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] #validation_score (10): 7.090169429779053\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] Timing: train: 0.75s, val: 0.06s, epoch: 0.81s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] #progress_metric: host=algo-2, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928970.9595113, \"EndTime\": 1701928971.771282, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 43480.0, \"count\": 1, \"min\": 43480, \"max\": 43480}, \"Total Batches Seen\": {\"sum\": 170.0, \"count\": 1, \"min\": 170, \"max\": 170}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=5355.063216866914 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:51 INFO 140221811656512] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:51.314] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 747, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] # Finished training epoch 11 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] Loss (name: value) total: 7.086120689616484\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] Loss (name: value) kld: 0.010752239757600953\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] Loss (name: value) recons: 7.075368460486917\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] Loss (name: value) logppx: 7.086120689616484\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=7.086120689616484\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:51.393] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 32, \"duration\": 77, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] Loss (name: value) total: 7.0960612297058105\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] Loss (name: value) kld: 0.012203427031636238\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] Loss (name: value) recons: 7.083857536315918\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] Loss (name: value) logppx: 7.0960612297058105\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] #validation_score (11): 7.0960612297058105\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] patience losses:[7.155533671379089, 7.139305472373962, 7.1346211433410645, 7.135199546813965, 7.116005301475525, 7.111607313156128, 7.1070709228515625, 7.1419419050216675, 7.1096824407577515, 7.102047443389893] min patience loss:7.102047443389893 current loss:7.0960612297058105 absolute loss difference:0.005986213684082031\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] Timing: train: 0.75s, val: 0.09s, epoch: 0.84s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928970.566483, \"EndTime\": 1701928971.4032495, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 47828.0, \"count\": 1, \"min\": 47828, \"max\": 47828}, \"Total Batches Seen\": {\"sum\": 187.0, \"count\": 1, \"min\": 187, \"max\": 187}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=5194.767468729135 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:51 INFO 140009233762112] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:52.282] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 510, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] # Finished training epoch 11 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) total: 7.084481603959027\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) kld: 0.017858577015645364\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) recons: 7.066623042611515\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) logppx: 7.084481603959027\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] #quality_metric: host=algo-2, epoch=11, train total_loss <loss>=7.084481603959027\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:52.331] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 32, \"duration\": 47, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) total: 7.086635231971741\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) kld: 0.013743703486397862\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) recons: 7.072891473770142\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) logppx: 7.086635231971741\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] #validation_score (11): 7.086635231971741\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] patience losses:[7.154886364936829, 7.1369030475616455, 7.139755487442017, 7.136035919189453, 7.114254474639893, 7.110749006271362, 7.129110813140869, 7.112606167793274, 7.096985578536987, 7.090169429779053] min patience loss:7.090169429779053 current loss:7.086635231971741 absolute loss difference:0.0035341978073120117\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Timing: train: 0.51s, val: 0.06s, epoch: 0.57s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] #progress_metric: host=algo-2, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928971.7716405, \"EndTime\": 1701928972.3403757, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 47828.0, \"count\": 1, \"min\": 47828, \"max\": 47828}, \"Total Batches Seen\": {\"sum\": 187.0, \"count\": 1, \"min\": 187, \"max\": 187}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7642.840441836226 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:52.847] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 504, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] # Finished training epoch 12 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) total: 7.071963394389433\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) kld: 0.017728328704833984\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) recons: 7.054235065684599\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) logppx: 7.071963394389433\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] #quality_metric: host=algo-2, epoch=12, train total_loss <loss>=7.071963394389433\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:52.071] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 664, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] # Finished training epoch 12 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) total: 7.083988526288201\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) kld: 0.015296245541642694\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) recons: 7.068692263434915\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) logppx: 7.083988526288201\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=7.083988526288201\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:52.142] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 35, \"duration\": 69, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) total: 7.095251441001892\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) kld: 0.01208987319841981\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) recons: 7.0831615924835205\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) logppx: 7.095251441001892\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] #validation_score (12): 7.095251441001892\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] patience losses:[7.139305472373962, 7.1346211433410645, 7.135199546813965, 7.116005301475525, 7.111607313156128, 7.1070709228515625, 7.1419419050216675, 7.1096824407577515, 7.102047443389893, 7.0960612297058105] min patience loss:7.0960612297058105 current loss:7.095251441001892 absolute loss difference:0.000809788703918457\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Timing: train: 0.67s, val: 0.08s, epoch: 0.75s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928971.4037302, \"EndTime\": 1701928972.150203, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 52176.0, \"count\": 1, \"min\": 52176, \"max\": 52176}, \"Total Batches Seen\": {\"sum\": 204.0, \"count\": 1, \"min\": 204, \"max\": 204}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=5823.035762648383 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:52.762] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 610, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] # Finished training epoch 13 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) total: 7.074655420639935\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) kld: 0.018824585987364546\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) recons: 7.055830787209904\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) logppx: 7.074655420639935\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=7.074655420639935\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:52.836] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 38, \"duration\": 72, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) total: 7.087842583656311\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) kld: 0.013220560504123569\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) recons: 7.074621915817261\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Loss (name: value) logppx: 7.087842583656311\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] #validation_score (13): 7.087842583656311\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] patience losses:[7.1346211433410645, 7.135199546813965, 7.116005301475525, 7.111607313156128, 7.1070709228515625, 7.1419419050216675, 7.1096824407577515, 7.102047443389893, 7.0960612297058105, 7.095251441001892] min patience loss:7.095251441001892 current loss:7.087842583656311 absolute loss difference:0.007408857345581055\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] Timing: train: 0.61s, val: 0.08s, epoch: 0.69s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928972.1506007, \"EndTime\": 1701928972.8427722, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 56524.0, \"count\": 1, \"min\": 56524, \"max\": 56524}, \"Total Batches Seen\": {\"sum\": 221.0, \"count\": 1, \"min\": 221, \"max\": 221}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=6277.38314071662 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:52 INFO 140009233762112] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:52.896] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 35, \"duration\": 47, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) total: 7.070450782775879\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) kld: 0.017100122291594744\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) recons: 7.053350806236267\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Loss (name: value) logppx: 7.070450782775879\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] #validation_score (12): 7.070450782775879\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] patience losses:[7.1369030475616455, 7.139755487442017, 7.136035919189453, 7.114254474639893, 7.110749006271362, 7.129110813140869, 7.112606167793274, 7.096985578536987, 7.090169429779053, 7.086635231971741] min patience loss:7.086635231971741 current loss:7.070450782775879 absolute loss difference:0.016184449195861816\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] Timing: train: 0.51s, val: 0.05s, epoch: 0.56s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] #progress_metric: host=algo-2, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928972.340691, \"EndTime\": 1701928972.9022448, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 52176.0, \"count\": 1, \"min\": 52176, \"max\": 52176}, \"Total Batches Seen\": {\"sum\": 204.0, \"count\": 1, \"min\": 204, \"max\": 204}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7740.56530877654 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:52 INFO 140221811656512] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:53.448] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 545, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] # Finished training epoch 13 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Loss (name: value) total: 7.061674370485194\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Loss (name: value) kld: 0.025183794143445352\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Loss (name: value) recons: 7.036490580614875\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Loss (name: value) logppx: 7.061674370485194\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] #quality_metric: host=algo-2, epoch=13, train total_loss <loss>=7.061674370485194\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:53.523] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 38, \"duration\": 72, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Loss (name: value) total: 7.099895596504211\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Loss (name: value) kld: 0.01748709799721837\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Loss (name: value) recons: 7.082408547401428\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Loss (name: value) logppx: 7.099895596504211\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] #validation_score (13): 7.099895596504211\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] patience losses:[7.139755487442017, 7.136035919189453, 7.114254474639893, 7.110749006271362, 7.129110813140869, 7.112606167793274, 7.096985578536987, 7.090169429779053, 7.086635231971741, 7.070450782775879] min patience loss:7.070450782775879 current loss:7.099895596504211 absolute loss difference:0.02944481372833252\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] Timing: train: 0.55s, val: 0.07s, epoch: 0.62s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] #progress_metric: host=algo-2, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928972.9025378, \"EndTime\": 1701928973.5251336, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 56524.0, \"count\": 1, \"min\": 56524, \"max\": 56524}, \"Total Batches Seen\": {\"sum\": 221.0, \"count\": 1, \"min\": 221, \"max\": 221}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=6981.843592567793 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:53 INFO 140221811656512] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:53.487] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 643, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] # Finished training epoch 14 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Loss (name: value) total: 7.061255370869356\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Loss (name: value) kld: 0.024711488800890306\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Loss (name: value) recons: 7.0365439022288605\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Loss (name: value) logppx: 7.061255370869356\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=7.061255370869356\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:53.555] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 41, \"duration\": 65, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Loss (name: value) total: 7.097676396369934\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Loss (name: value) kld: 0.016646632459014654\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Loss (name: value) recons: 7.081030011177063\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Loss (name: value) logppx: 7.097676396369934\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] #validation_score (14): 7.097676396369934\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] patience losses:[7.135199546813965, 7.116005301475525, 7.111607313156128, 7.1070709228515625, 7.1419419050216675, 7.1096824407577515, 7.102047443389893, 7.0960612297058105, 7.095251441001892, 7.087842583656311] min patience loss:7.087842583656311 current loss:7.097676396369934 absolute loss difference:0.009833812713623047\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] Timing: train: 0.65s, val: 0.07s, epoch: 0.71s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928972.8435385, \"EndTime\": 1701928973.5566406, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 60872.0, \"count\": 1, \"min\": 60872, \"max\": 60872}, \"Total Batches Seen\": {\"sum\": 238.0, \"count\": 1, \"min\": 238, \"max\": 238}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=6095.746506621083 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:53 INFO 140009233762112] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:54.005] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 479, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] # Finished training epoch 14 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) total: 7.0598483927109665\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) kld: 0.03099271364729194\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) recons: 7.02885560428395\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) logppx: 7.0598483927109665\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] #quality_metric: host=algo-2, epoch=14, train total_loss <loss>=7.0598483927109665\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:54.054] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 41, \"duration\": 46, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) total: 7.050808310508728\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) kld: 0.023005782160907984\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) recons: 7.027802467346191\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) logppx: 7.050808310508728\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] #validation_score (14): 7.050808310508728\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] patience losses:[7.136035919189453, 7.114254474639893, 7.110749006271362, 7.129110813140869, 7.112606167793274, 7.096985578536987, 7.090169429779053, 7.086635231971741, 7.070450782775879, 7.099895596504211] min patience loss:7.070450782775879 current loss:7.050808310508728 absolute loss difference:0.01964247226715088\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Timing: train: 0.48s, val: 0.05s, epoch: 0.53s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] #progress_metric: host=algo-2, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928973.525494, \"EndTime\": 1701928974.0607803, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 60872.0, \"count\": 1, \"min\": 60872, \"max\": 60872}, \"Total Batches Seen\": {\"sum\": 238.0, \"count\": 1, \"min\": 238, \"max\": 238}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=8120.103011415979 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:54.542] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 480, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] # Finished training epoch 15 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) total: 7.047448775347541\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) kld: 0.022769888589049086\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) recons: 7.02467887541827\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) logppx: 7.047448775347541\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] #quality_metric: host=algo-2, epoch=15, train total_loss <loss>=7.047448775347541\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:54.601] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 44, \"duration\": 57, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) total: 7.054351806640625\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) kld: 0.049065034836530685\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) recons: 7.005286931991577\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Loss (name: value) logppx: 7.054351806640625\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] #validation_score (15): 7.054351806640625\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] patience losses:[7.114254474639893, 7.110749006271362, 7.129110813140869, 7.112606167793274, 7.096985578536987, 7.090169429779053, 7.086635231971741, 7.070450782775879, 7.099895596504211, 7.050808310508728] min patience loss:7.050808310508728 current loss:7.054351806640625 absolute loss difference:0.0035434961318969727\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] Timing: train: 0.48s, val: 0.06s, epoch: 0.54s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] #progress_metric: host=algo-2, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928974.0611348, \"EndTime\": 1701928974.603104, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 65220.0, \"count\": 1, \"min\": 65220, \"max\": 65220}, \"Total Batches Seen\": {\"sum\": 255.0, \"count\": 1, \"min\": 255, \"max\": 255}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=8020.26955774871 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:54 INFO 140221811656512] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:54.057] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 498, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] # Finished training epoch 15 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) total: 7.064065456390381\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) kld: 0.032220085918465084\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) recons: 7.031845373265884\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) logppx: 7.064065456390381\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=7.064065456390381\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:54.105] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 44, \"duration\": 47, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) total: 7.078240752220154\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) kld: 0.017649314366281033\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) recons: 7.060591340065002\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) logppx: 7.078240752220154\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] #validation_score (15): 7.078240752220154\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] patience losses:[7.116005301475525, 7.111607313156128, 7.1070709228515625, 7.1419419050216675, 7.1096824407577515, 7.102047443389893, 7.0960612297058105, 7.095251441001892, 7.087842583656311, 7.097676396369934] min patience loss:7.087842583656311 current loss:7.078240752220154 absolute loss difference:0.009601831436157227\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Timing: train: 0.50s, val: 0.05s, epoch: 0.55s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928973.5569572, \"EndTime\": 1701928974.1109743, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 65220.0, \"count\": 1, \"min\": 65220, \"max\": 65220}, \"Total Batches Seen\": {\"sum\": 255.0, \"count\": 1, \"min\": 255, \"max\": 255}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7844.744223820503 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:54.584] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 468, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] # Finished training epoch 16 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) total: 7.0516747586867385\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) kld: 0.027066842652857304\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) recons: 7.024607938878677\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) logppx: 7.0516747586867385\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=7.0516747586867385\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:54.628] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 47, \"duration\": 42, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) total: 7.053735256195068\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) kld: 0.05106284748762846\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) recons: 7.00267231464386\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Loss (name: value) logppx: 7.053735256195068\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] #validation_score (16): 7.053735256195068\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] patience losses:[7.111607313156128, 7.1070709228515625, 7.1419419050216675, 7.1096824407577515, 7.102047443389893, 7.0960612297058105, 7.095251441001892, 7.087842583656311, 7.097676396369934, 7.078240752220154] min patience loss:7.078240752220154 current loss:7.053735256195068 absolute loss difference:0.02450549602508545\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] Timing: train: 0.47s, val: 0.05s, epoch: 0.52s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928974.1147459, \"EndTime\": 1701928974.6348057, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 69568.0, \"count\": 1, \"min\": 69568, \"max\": 69568}, \"Total Batches Seen\": {\"sum\": 272.0, \"count\": 1, \"min\": 272, \"max\": 272}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8357.79104827829 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:54 INFO 140009233762112] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:55.135] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 531, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] # Finished training epoch 16 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) total: 7.02022398219389\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) kld: 0.035893619389218444\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) recons: 6.9843303175533515\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) logppx: 7.02022398219389\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] #quality_metric: host=algo-2, epoch=16, train total_loss <loss>=7.02022398219389\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:55.200] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 47, \"duration\": 63, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) total: 7.017643928527832\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) kld: 0.04339228104799986\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) recons: 6.974251627922058\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) logppx: 7.017643928527832\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] #validation_score (16): 7.017643928527832\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] patience losses:[7.110749006271362, 7.129110813140869, 7.112606167793274, 7.096985578536987, 7.090169429779053, 7.086635231971741, 7.070450782775879, 7.099895596504211, 7.050808310508728, 7.054351806640625] min patience loss:7.050808310508728 current loss:7.017643928527832 absolute loss difference:0.033164381980895996\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Timing: train: 0.53s, val: 0.07s, epoch: 0.60s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] #progress_metric: host=algo-2, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928974.6034417, \"EndTime\": 1701928975.2056653, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 69568.0, \"count\": 1, \"min\": 69568, \"max\": 69568}, \"Total Batches Seen\": {\"sum\": 272.0, \"count\": 1, \"min\": 272, \"max\": 272}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7217.66616456128 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:55.702] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 496, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] # Finished training epoch 17 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) total: 6.998800782596364\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) kld: 0.04186427352183005\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) recons: 6.956936527700985\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) logppx: 6.998800782596364\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] #quality_metric: host=algo-2, epoch=17, train total_loss <loss>=6.998800782596364\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:55.746] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 50, \"duration\": 41, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) total: 7.00554358959198\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) kld: 0.044950541108846664\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) recons: 6.960593104362488\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Loss (name: value) logppx: 7.00554358959198\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] #validation_score (17): 7.00554358959198\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] patience losses:[7.129110813140869, 7.112606167793274, 7.096985578536987, 7.090169429779053, 7.086635231971741, 7.070450782775879, 7.099895596504211, 7.050808310508728, 7.054351806640625, 7.017643928527832] min patience loss:7.017643928527832 current loss:7.00554358959198 absolute loss difference:0.01210033893585205\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] Timing: train: 0.50s, val: 0.05s, epoch: 0.55s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] #progress_metric: host=algo-2, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928975.2060387, \"EndTime\": 1701928975.7531524, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 73916.0, \"count\": 1, \"min\": 73916, \"max\": 73916}, \"Total Batches Seen\": {\"sum\": 289.0, \"count\": 1, \"min\": 289, \"max\": 289}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7944.6988645472475 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:55 INFO 140221811656512] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:55.134] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 498, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] # Finished training epoch 17 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) total: 7.0212474430308625\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) kld: 0.03630243099349387\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) recons: 6.9849450447980095\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) logppx: 7.0212474430308625\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=7.0212474430308625\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:55.197] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 50, \"duration\": 61, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) total: 7.020589113235474\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) kld: 0.04344274755567312\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) recons: 6.977146506309509\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) logppx: 7.020589113235474\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] #validation_score (17): 7.020589113235474\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] patience losses:[7.1070709228515625, 7.1419419050216675, 7.1096824407577515, 7.102047443389893, 7.0960612297058105, 7.095251441001892, 7.087842583656311, 7.097676396369934, 7.078240752220154, 7.053735256195068] min patience loss:7.053735256195068 current loss:7.020589113235474 absolute loss difference:0.03314614295959473\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Timing: train: 0.50s, val: 0.07s, epoch: 0.57s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928974.635247, \"EndTime\": 1701928975.20523, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 73916.0, \"count\": 1, \"min\": 73916, \"max\": 73916}, \"Total Batches Seen\": {\"sum\": 289.0, \"count\": 1, \"min\": 289, \"max\": 289}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7626.1890460716 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:55.685] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 478, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] # Finished training epoch 18 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) total: 6.994571826037238\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) kld: 0.04335322143400416\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) recons: 6.951218661139993\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) logppx: 6.994571826037238\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=6.994571826037238\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:55.736] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 53, \"duration\": 49, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) total: 7.008104205131531\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) kld: 0.04345571529120207\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) recons: 6.964648365974426\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Loss (name: value) logppx: 7.008104205131531\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] #validation_score (18): 7.008104205131531\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] patience losses:[7.1419419050216675, 7.1096824407577515, 7.102047443389893, 7.0960612297058105, 7.095251441001892, 7.087842583656311, 7.097676396369934, 7.078240752220154, 7.053735256195068, 7.020589113235474] min patience loss:7.020589113235474 current loss:7.008104205131531 absolute loss difference:0.012484908103942871\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] Timing: train: 0.48s, val: 0.05s, epoch: 0.53s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928975.2055798, \"EndTime\": 1701928975.7407281, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 78264.0, \"count\": 1, \"min\": 78264, \"max\": 78264}, \"Total Batches Seen\": {\"sum\": 306.0, \"count\": 1, \"min\": 306, \"max\": 306}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8122.497207184135 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:55 INFO 140009233762112] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:56.247] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 493, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] # Finished training epoch 18 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) total: 6.98287046656889\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) kld: 0.04594420488266384\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) recons: 6.936926196603214\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) logppx: 6.98287046656889\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] #quality_metric: host=algo-2, epoch=18, train total_loss <loss>=6.98287046656889\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:56.292] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 53, \"duration\": 44, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) total: 6.994876742362976\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) kld: 0.044902573339641094\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) recons: 6.949974298477173\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) logppx: 6.994876742362976\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] #validation_score (18): 6.994876742362976\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] patience losses:[7.112606167793274, 7.096985578536987, 7.090169429779053, 7.086635231971741, 7.070450782775879, 7.099895596504211, 7.050808310508728, 7.054351806640625, 7.017643928527832, 7.00554358959198] min patience loss:7.00554358959198 current loss:6.994876742362976 absolute loss difference:0.010666847229003906\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Timing: train: 0.49s, val: 0.05s, epoch: 0.54s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] #progress_metric: host=algo-2, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928975.7535353, \"EndTime\": 1701928976.2988038, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 78264.0, \"count\": 1, \"min\": 78264, \"max\": 78264}, \"Total Batches Seen\": {\"sum\": 306.0, \"count\": 1, \"min\": 306, \"max\": 306}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7971.658017693684 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:56.806] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 507, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] # Finished training epoch 19 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) total: 6.978038984186509\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) kld: 0.04921218555639772\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) recons: 6.928826836978688\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) logppx: 6.978038984186509\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] #quality_metric: host=algo-2, epoch=19, train total_loss <loss>=6.978038984186509\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:56.852] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 56, \"duration\": 44, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) total: 7.005527019500732\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) kld: 0.04852037783712149\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) recons: 6.957006454467773\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Loss (name: value) logppx: 7.005527019500732\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] #validation_score (19): 7.005527019500732\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] patience losses:[7.096985578536987, 7.090169429779053, 7.086635231971741, 7.070450782775879, 7.099895596504211, 7.050808310508728, 7.054351806640625, 7.017643928527832, 7.00554358959198, 6.994876742362976] min patience loss:6.994876742362976 current loss:7.005527019500732 absolute loss difference:0.010650277137756348\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] Timing: train: 0.51s, val: 0.05s, epoch: 0.55s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] #progress_metric: host=algo-2, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928976.2990887, \"EndTime\": 1701928976.8539162, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 82612.0, \"count\": 1, \"min\": 82612, \"max\": 82612}, \"Total Batches Seen\": {\"sum\": 323.0, \"count\": 1, \"min\": 323, \"max\": 323}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7834.49580605569 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:56 INFO 140221811656512] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:56.231] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 489, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] # Finished training epoch 19 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) total: 6.9812913782456345\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) kld: 0.04720011594540933\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) recons: 6.934091287500718\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) logppx: 6.9812913782456345\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=6.9812913782456345\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:56.275] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 56, \"duration\": 42, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) total: 6.99849534034729\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) kld: 0.04511789605021477\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) recons: 6.953377366065979\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) logppx: 6.99849534034729\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] #validation_score (19): 6.99849534034729\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] patience losses:[7.1096824407577515, 7.102047443389893, 7.0960612297058105, 7.095251441001892, 7.087842583656311, 7.097676396369934, 7.078240752220154, 7.053735256195068, 7.020589113235474, 7.008104205131531] min patience loss:7.008104205131531 current loss:6.99849534034729 absolute loss difference:0.009608864784240723\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Timing: train: 0.49s, val: 0.05s, epoch: 0.54s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928975.7410436, \"EndTime\": 1701928976.2820754, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 82612.0, \"count\": 1, \"min\": 82612, \"max\": 82612}, \"Total Batches Seen\": {\"sum\": 323.0, \"count\": 1, \"min\": 323, \"max\": 323}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8034.070457462035 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:56.780] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 497, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] # Finished training epoch 20 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) total: 6.976425535538617\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) kld: 0.05046413466334343\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) recons: 6.925961354199578\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) logppx: 6.976425535538617\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=6.976425535538617\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:56.844] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 59, \"duration\": 61, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) total: 6.9979143142700195\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) kld: 0.04665534012019634\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) recons: 6.951259016990662\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Loss (name: value) logppx: 6.9979143142700195\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] #validation_score (20): 6.9979143142700195\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] patience losses:[7.102047443389893, 7.0960612297058105, 7.095251441001892, 7.087842583656311, 7.097676396369934, 7.078240752220154, 7.053735256195068, 7.020589113235474, 7.008104205131531, 6.99849534034729] min patience loss:6.99849534034729 current loss:6.9979143142700195 absolute loss difference:0.0005810260772705078\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] Timing: train: 0.50s, val: 0.07s, epoch: 0.57s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928976.2824256, \"EndTime\": 1701928976.8493803, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 86960.0, \"count\": 1, \"min\": 86960, \"max\": 86960}, \"Total Batches Seen\": {\"sum\": 340.0, \"count\": 1, \"min\": 340, \"max\": 340}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7666.809936464575 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:56 INFO 140009233762112] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:57.382] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 528, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] # Finished training epoch 20 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Loss (name: value) total: 6.980985192691579\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Loss (name: value) kld: 0.0545696657808388\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Loss (name: value) recons: 6.926415527568144\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Loss (name: value) logppx: 6.980985192691579\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] #quality_metric: host=algo-2, epoch=20, train total_loss <loss>=6.980985192691579\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:57.433] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 59, \"duration\": 49, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Loss (name: value) total: 7.009495973587036\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Loss (name: value) kld: 0.042354995384812355\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Loss (name: value) recons: 6.9671409130096436\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Loss (name: value) logppx: 7.009495973587036\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] #validation_score (20): 7.009495973587036\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] patience losses:[7.090169429779053, 7.086635231971741, 7.070450782775879, 7.099895596504211, 7.050808310508728, 7.054351806640625, 7.017643928527832, 7.00554358959198, 6.994876742362976, 7.005527019500732] min patience loss:6.994876742362976 current loss:7.009495973587036 absolute loss difference:0.014619231224060059\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Timing: train: 0.53s, val: 0.05s, epoch: 0.58s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] #progress_metric: host=algo-2, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928976.854231, \"EndTime\": 1701928977.435271, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 86960.0, \"count\": 1, \"min\": 86960, \"max\": 86960}, \"Total Batches Seen\": {\"sum\": 340.0, \"count\": 1, \"min\": 340, \"max\": 340}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7480.94623818234 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:57.338] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 488, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] # Finished training epoch 21 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) total: 6.982224296121037\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) kld: 0.054977714358007204\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) recons: 6.927246514488669\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) logppx: 6.982224296121037\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=6.982224296121037\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:57.383] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 62, \"duration\": 42, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) total: 6.9849231243133545\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) kld: 0.04239104501903057\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) recons: 6.942532062530518\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) logppx: 6.9849231243133545\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] #validation_score (21): 6.9849231243133545\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] patience losses:[7.0960612297058105, 7.095251441001892, 7.087842583656311, 7.097676396369934, 7.078240752220154, 7.053735256195068, 7.020589113235474, 7.008104205131531, 6.99849534034729, 6.9979143142700195] min patience loss:6.9979143142700195 current loss:6.9849231243133545 absolute loss difference:0.012991189956665039\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Timing: train: 0.49s, val: 0.05s, epoch: 0.54s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928976.8496554, \"EndTime\": 1701928977.3890622, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 91308.0, \"count\": 1, \"min\": 91308, \"max\": 91308}, \"Total Batches Seen\": {\"sum\": 357.0, \"count\": 1, \"min\": 357, \"max\": 357}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8058.719462800166 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:57.981] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 545, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] # Finished training epoch 21 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Loss (name: value) total: 6.9743873091305\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Loss (name: value) kld: 0.051380642196711374\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Loss (name: value) recons: 6.923006758970373\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] Loss (name: value) logppx: 6.9743873091305\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:57 INFO 140221811656512] #quality_metric: host=algo-2, epoch=21, train total_loss <loss>=6.9743873091305\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:58.025] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 62, \"duration\": 42, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Loss (name: value) total: 6.984156370162964\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Loss (name: value) kld: 0.0648453189060092\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Loss (name: value) recons: 6.919311046600342\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Loss (name: value) logppx: 6.984156370162964\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] #validation_score (21): 6.984156370162964\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] patience losses:[7.086635231971741, 7.070450782775879, 7.099895596504211, 7.050808310508728, 7.054351806640625, 7.017643928527832, 7.00554358959198, 6.994876742362976, 7.005527019500732, 7.009495973587036] min patience loss:6.994876742362976 current loss:6.984156370162964 absolute loss difference:0.010720372200012207\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Timing: train: 0.55s, val: 0.05s, epoch: 0.60s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] #progress_metric: host=algo-2, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928977.4355888, \"EndTime\": 1701928978.0325074, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 91308.0, \"count\": 1, \"min\": 91308, \"max\": 91308}, \"Total Batches Seen\": {\"sum\": 357.0, \"count\": 1, \"min\": 357, \"max\": 357}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7280.981199510365 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:58.633] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 598, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] # Finished training epoch 22 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Loss (name: value) total: 6.959456920623779\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Loss (name: value) kld: 0.054845272837316286\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Loss (name: value) recons: 6.904611587524414\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Loss (name: value) logppx: 6.959456920623779\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] #quality_metric: host=algo-2, epoch=22, train total_loss <loss>=6.959456920623779\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:58.701] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 65, \"duration\": 65, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Loss (name: value) total: 6.9618765115737915\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Loss (name: value) kld: 0.052324721589684486\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Loss (name: value) recons: 6.909551739692688\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Loss (name: value) logppx: 6.9618765115737915\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] #validation_score (22): 6.9618765115737915\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] patience losses:[7.070450782775879, 7.099895596504211, 7.050808310508728, 7.054351806640625, 7.017643928527832, 7.00554358959198, 6.994876742362976, 7.005527019500732, 7.009495973587036, 6.984156370162964] min patience loss:6.984156370162964 current loss:6.9618765115737915 absolute loss difference:0.022279858589172363\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] Timing: train: 0.60s, val: 0.07s, epoch: 0.67s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] #progress_metric: host=algo-2, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928978.0329037, \"EndTime\": 1701928978.705788, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 95656.0, \"count\": 1, \"min\": 95656, \"max\": 95656}, \"Total Batches Seen\": {\"sum\": 374.0, \"count\": 1, \"min\": 374, \"max\": 374}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=6459.776409008844 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:58 INFO 140221811656512] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:57.926] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 537, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] # Finished training epoch 22 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) total: 6.9756231588475845\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) kld: 0.04962288456804612\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) recons: 6.926000342649572\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) logppx: 6.9756231588475845\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=6.9756231588475845\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:57.996] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 65, \"duration\": 67, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) total: 6.9890265464782715\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) kld: 0.06597046181559563\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) recons: 6.92305600643158\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Loss (name: value) logppx: 6.9890265464782715\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] #validation_score (22): 6.9890265464782715\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] patience losses:[7.095251441001892, 7.087842583656311, 7.097676396369934, 7.078240752220154, 7.053735256195068, 7.020589113235474, 7.008104205131531, 6.99849534034729, 6.9979143142700195, 6.9849231243133545] min patience loss:6.9849231243133545 current loss:6.9890265464782715 absolute loss difference:0.004103422164916992\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] Timing: train: 0.54s, val: 0.07s, epoch: 0.61s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928977.3893225, \"EndTime\": 1701928977.9982855, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 95656.0, \"count\": 1, \"min\": 95656, \"max\": 95656}, \"Total Batches Seen\": {\"sum\": 374.0, \"count\": 1, \"min\": 374, \"max\": 374}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7138.2544141368235 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:57 INFO 140009233762112] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:58.550] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 551, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] # Finished training epoch 23 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] Loss (name: value) total: 6.956472817589255\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] Loss (name: value) kld: 0.05612165217890459\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] Loss (name: value) recons: 6.900351243860581\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] Loss (name: value) logppx: 6.956472817589255\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=6.956472817589255\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:58.601] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 68, \"duration\": 49, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] Loss (name: value) total: 6.966446399688721\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] Loss (name: value) kld: 0.05216711200773716\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] Loss (name: value) recons: 6.914279222488403\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] Loss (name: value) logppx: 6.966446399688721\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] #validation_score (23): 6.966446399688721\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] patience losses:[7.087842583656311, 7.097676396369934, 7.078240752220154, 7.053735256195068, 7.020589113235474, 7.008104205131531, 6.99849534034729, 6.9979143142700195, 6.9849231243133545, 6.9890265464782715] min patience loss:6.9849231243133545 current loss:6.966446399688721 absolute loss difference:0.01847672462463379\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] Timing: train: 0.55s, val: 0.05s, epoch: 0.61s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928977.9985886, \"EndTime\": 1701928978.606544, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 100004.0, \"count\": 1, \"min\": 100004, \"max\": 100004}, \"Total Batches Seen\": {\"sum\": 391.0, \"count\": 1, \"min\": 391, \"max\": 391}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7149.947088262681 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:58 INFO 140009233762112] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:59.251] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 544, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] # Finished training epoch 23 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) total: 6.939568603740019\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) kld: 0.057648022604339266\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) recons: 6.881920562070959\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) logppx: 6.939568603740019\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] #quality_metric: host=algo-2, epoch=23, train total_loss <loss>=6.939568603740019\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:59.316] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 68, \"duration\": 62, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) total: 6.9559714794158936\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) kld: 0.05462375655770302\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) recons: 6.901347637176514\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) logppx: 6.9559714794158936\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] #validation_score (23): 6.9559714794158936\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] patience losses:[7.099895596504211, 7.050808310508728, 7.054351806640625, 7.017643928527832, 7.00554358959198, 6.994876742362976, 7.005527019500732, 7.009495973587036, 6.984156370162964, 6.9618765115737915] min patience loss:6.9618765115737915 current loss:6.9559714794158936 absolute loss difference:0.005905032157897949\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Timing: train: 0.55s, val: 0.07s, epoch: 0.62s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] #progress_metric: host=algo-2, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928978.7061539, \"EndTime\": 1701928979.3234606, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 100004.0, \"count\": 1, \"min\": 100004, \"max\": 100004}, \"Total Batches Seen\": {\"sum\": 391.0, \"count\": 1, \"min\": 391, \"max\": 391}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7037.797601948079 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:59.858] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 534, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] # Finished training epoch 24 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) total: 6.929723318885355\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) kld: 0.05659804339794552\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) recons: 6.873125244589413\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) logppx: 6.929723318885355\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] #quality_metric: host=algo-2, epoch=24, train total_loss <loss>=6.929723318885355\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:59.213] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 606, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] # Finished training epoch 24 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) total: 6.942815780639648\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) kld: 0.059094261158915126\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) recons: 6.883721604066737\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) logppx: 6.942815780639648\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=6.942815780639648\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:59.261] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 71, \"duration\": 46, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) total: 6.952747225761414\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) kld: 0.05923168547451496\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) recons: 6.893515586853027\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) logppx: 6.952747225761414\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] #validation_score (24): 6.952747225761414\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] patience losses:[7.097676396369934, 7.078240752220154, 7.053735256195068, 7.020589113235474, 7.008104205131531, 6.99849534034729, 6.9979143142700195, 6.9849231243133545, 6.9890265464782715, 6.966446399688721] min patience loss:6.966446399688721 current loss:6.952747225761414 absolute loss difference:0.013699173927307129\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Timing: train: 0.61s, val: 0.05s, epoch: 0.66s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928978.6069152, \"EndTime\": 1701928979.2681649, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 104352.0, \"count\": 1, \"min\": 104352, \"max\": 104352}, \"Total Batches Seen\": {\"sum\": 408.0, \"count\": 1, \"min\": 408, \"max\": 408}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=6573.821910776595 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:59.788] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 519, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] # Finished training epoch 25 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) total: 6.9328634037691\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) kld: 0.056403718012220716\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) recons: 6.876459682688994\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) logppx: 6.9328634037691\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=6.9328634037691\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:02:59.858] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 74, \"duration\": 68, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) total: 6.949137091636658\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) kld: 0.06508554518222809\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) recons: 6.884051561355591\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Loss (name: value) logppx: 6.949137091636658\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] #validation_score (25): 6.949137091636658\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] patience losses:[7.078240752220154, 7.053735256195068, 7.020589113235474, 7.008104205131531, 6.99849534034729, 6.9979143142700195, 6.9849231243133545, 6.9890265464782715, 6.966446399688721, 6.952747225761414] min patience loss:6.952747225761414 current loss:6.949137091636658 absolute loss difference:0.0036101341247558594\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] Timing: train: 0.52s, val: 0.07s, epoch: 0.60s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928979.2684803, \"EndTime\": 1701928979.8643181, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 108700.0, \"count\": 1, \"min\": 108700, \"max\": 108700}, \"Total Batches Seen\": {\"sum\": 425.0, \"count\": 1, \"min\": 425, \"max\": 425}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7295.3375707508585 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:02:59 INFO 140009233762112] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:02:59.923] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 71, \"duration\": 62, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) total: 6.954965949058533\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) kld: 0.06877939030528069\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) recons: 6.886186599731445\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Loss (name: value) logppx: 6.954965949058533\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] #validation_score (24): 6.954965949058533\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] patience losses:[7.050808310508728, 7.054351806640625, 7.017643928527832, 7.00554358959198, 6.994876742362976, 7.005527019500732, 7.009495973587036, 6.984156370162964, 6.9618765115737915, 6.9559714794158936] min patience loss:6.9559714794158936 current loss:6.954965949058533 absolute loss difference:0.0010055303573608398\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] Timing: train: 0.54s, val: 0.07s, epoch: 0.60s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] #progress_metric: host=algo-2, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928979.3243759, \"EndTime\": 1701928979.9289265, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 104352.0, \"count\": 1, \"min\": 104352, \"max\": 104352}, \"Total Batches Seen\": {\"sum\": 408.0, \"count\": 1, \"min\": 408, \"max\": 408}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7189.573069428089 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:02:59 INFO 140221811656512] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:00.428] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 498, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] # Finished training epoch 25 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Loss (name: value) total: 6.921858338748708\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Loss (name: value) kld: 0.06083170938141206\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Loss (name: value) recons: 6.861026595620548\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Loss (name: value) logppx: 6.921858338748708\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] #quality_metric: host=algo-2, epoch=25, train total_loss <loss>=6.921858338748708\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:00.488] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 74, \"duration\": 59, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Loss (name: value) total: 6.936262011528015\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Loss (name: value) kld: 0.0630352832376957\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Loss (name: value) recons: 6.873226761817932\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Loss (name: value) logppx: 6.936262011528015\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] #validation_score (25): 6.936262011528015\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] patience losses:[7.054351806640625, 7.017643928527832, 7.00554358959198, 6.994876742362976, 7.005527019500732, 7.009495973587036, 6.984156370162964, 6.9618765115737915, 6.9559714794158936, 6.954965949058533] min patience loss:6.954965949058533 current loss:6.936262011528015 absolute loss difference:0.018703937530517578\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Timing: train: 0.50s, val: 0.07s, epoch: 0.57s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] #progress_metric: host=algo-2, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928979.9293847, \"EndTime\": 1701928980.4957929, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 108700.0, \"count\": 1, \"min\": 108700, \"max\": 108700}, \"Total Batches Seen\": {\"sum\": 425.0, \"count\": 1, \"min\": 425, \"max\": 425}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7674.617892960051 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:00.332] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 468, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] # Finished training epoch 26 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) total: 6.923563704771154\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) kld: 0.06268172689220484\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) recons: 6.8608819456661445\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) logppx: 6.923563704771154\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=6.923563704771154\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:00.379] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 77, \"duration\": 44, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) total: 6.949759006500244\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) kld: 0.062049868516623974\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) recons: 6.887709140777588\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) logppx: 6.949759006500244\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] #validation_score (26): 6.949759006500244\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] patience losses:[7.053735256195068, 7.020589113235474, 7.008104205131531, 6.99849534034729, 6.9979143142700195, 6.9849231243133545, 6.9890265464782715, 6.966446399688721, 6.952747225761414, 6.949137091636658] min patience loss:6.949137091636658 current loss:6.949759006500244 absolute loss difference:0.0006219148635864258\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Timing: train: 0.47s, val: 0.05s, epoch: 0.52s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928979.8646345, \"EndTime\": 1701928980.3817327, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 113048.0, \"count\": 1, \"min\": 113048, \"max\": 113048}, \"Total Batches Seen\": {\"sum\": 442.0, \"count\": 1, \"min\": 442, \"max\": 442}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8406.027113222202 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:00.864] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 482, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] # Finished training epoch 27 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) total: 6.912864460664637\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) kld: 0.06447831563213292\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) recons: 6.848386063295252\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) logppx: 6.912864460664637\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=6.912864460664637\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:00.968] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 471, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] # Finished training epoch 26 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Loss (name: value) total: 6.908047311446246\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Loss (name: value) kld: 0.06402834482929286\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Loss (name: value) recons: 6.844018880058737\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] Loss (name: value) logppx: 6.908047311446246\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:00 INFO 140221811656512] #quality_metric: host=algo-2, epoch=26, train total_loss <loss>=6.908047311446246\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:01.034] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 77, \"duration\": 64, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Loss (name: value) total: 6.933321714401245\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Loss (name: value) kld: 0.055123441852629185\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Loss (name: value) recons: 6.8781983852386475\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Loss (name: value) logppx: 6.933321714401245\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] #validation_score (26): 6.933321714401245\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] patience losses:[7.017643928527832, 7.00554358959198, 6.994876742362976, 7.005527019500732, 7.009495973587036, 6.984156370162964, 6.9618765115737915, 6.9559714794158936, 6.954965949058533, 6.936262011528015] min patience loss:6.936262011528015 current loss:6.933321714401245 absolute loss difference:0.0029402971267700195\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Timing: train: 0.47s, val: 0.07s, epoch: 0.54s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] #progress_metric: host=algo-2, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928980.4961083, \"EndTime\": 1701928981.0398295, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 113048.0, \"count\": 1, \"min\": 113048, \"max\": 113048}, \"Total Batches Seen\": {\"sum\": 442.0, \"count\": 1, \"min\": 442, \"max\": 442}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7994.217998617427 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:01.560] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 520, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] # Finished training epoch 27 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Loss (name: value) total: 6.897244341233197\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Loss (name: value) kld: 0.06364002275992842\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Loss (name: value) recons: 6.833604307735667\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Loss (name: value) logppx: 6.897244341233197\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] #quality_metric: host=algo-2, epoch=27, train total_loss <loss>=6.897244341233197\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:01.631] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 80, \"duration\": 68, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Loss (name: value) total: 6.930997610092163\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Loss (name: value) kld: 0.07603907771408558\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Loss (name: value) recons: 6.854958653450012\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Loss (name: value) logppx: 6.930997610092163\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] #validation_score (27): 6.930997610092163\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] patience losses:[7.00554358959198, 6.994876742362976, 7.005527019500732, 7.009495973587036, 6.984156370162964, 6.9618765115737915, 6.9559714794158936, 6.954965949058533, 6.936262011528015, 6.933321714401245] min patience loss:6.933321714401245 current loss:6.930997610092163 absolute loss difference:0.0023241043090820312\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] Timing: train: 0.52s, val: 0.07s, epoch: 0.60s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] #progress_metric: host=algo-2, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928981.0401218, \"EndTime\": 1701928981.6374063, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 117396.0, \"count\": 1, \"min\": 117396, \"max\": 117396}, \"Total Batches Seen\": {\"sum\": 459.0, \"count\": 1, \"min\": 459, \"max\": 459}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7277.558478790055 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:01 INFO 140221811656512] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:00.914] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 80, \"duration\": 49, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) total: 6.939121603965759\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) kld: 0.07291154377162457\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) recons: 6.866209983825684\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Loss (name: value) logppx: 6.939121603965759\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] #validation_score (27): 6.939121603965759\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] patience losses:[7.020589113235474, 7.008104205131531, 6.99849534034729, 6.9979143142700195, 6.9849231243133545, 6.9890265464782715, 6.966446399688721, 6.952747225761414, 6.949137091636658, 6.949759006500244] min patience loss:6.949137091636658 current loss:6.939121603965759 absolute loss difference:0.010015487670898438\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] Timing: train: 0.48s, val: 0.05s, epoch: 0.54s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928980.3820295, \"EndTime\": 1701928980.9198365, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 117396.0, \"count\": 1, \"min\": 117396, \"max\": 117396}, \"Total Batches Seen\": {\"sum\": 459.0, \"count\": 1, \"min\": 459, \"max\": 459}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8082.323624736026 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:00 INFO 140009233762112] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:01.531] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 610, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] # Finished training epoch 28 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] Loss (name: value) total: 6.901699262506821\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] Loss (name: value) kld: 0.06226734685547212\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] Loss (name: value) recons: 6.839431987089269\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] Loss (name: value) logppx: 6.901699262506821\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=6.901699262506821\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:01.587] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 83, \"duration\": 53, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] Loss (name: value) total: 6.931114912033081\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] Loss (name: value) kld: 0.07519916072487831\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] Loss (name: value) recons: 6.855915784835815\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] Loss (name: value) logppx: 6.931114912033081\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] #validation_score (28): 6.931114912033081\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] patience losses:[7.008104205131531, 6.99849534034729, 6.9979143142700195, 6.9849231243133545, 6.9890265464782715, 6.966446399688721, 6.952747225761414, 6.949137091636658, 6.949759006500244, 6.939121603965759] min patience loss:6.939121603965759 current loss:6.931114912033081 absolute loss difference:0.008006691932678223\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] Timing: train: 0.61s, val: 0.06s, epoch: 0.67s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928980.9201317, \"EndTime\": 1701928981.593003, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 121744.0, \"count\": 1, \"min\": 121744, \"max\": 121744}, \"Total Batches Seen\": {\"sum\": 476.0, \"count\": 1, \"min\": 476, \"max\": 476}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=6460.575074004849 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:01 INFO 140009233762112] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:02.243] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 605, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] # Finished training epoch 28 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Loss (name: value) total: 6.888496567221249\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Loss (name: value) kld: 0.06389711490448784\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Loss (name: value) recons: 6.82459935019998\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Loss (name: value) logppx: 6.888496567221249\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] #quality_metric: host=algo-2, epoch=28, train total_loss <loss>=6.888496567221249\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:02.291] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 83, \"duration\": 46, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Loss (name: value) total: 6.911644101142883\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Loss (name: value) kld: 0.06523406319320202\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Loss (name: value) recons: 6.846409916877747\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Loss (name: value) logppx: 6.911644101142883\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] #validation_score (28): 6.911644101142883\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] patience losses:[6.994876742362976, 7.005527019500732, 7.009495973587036, 6.984156370162964, 6.9618765115737915, 6.9559714794158936, 6.954965949058533, 6.936262011528015, 6.933321714401245, 6.930997610092163] min patience loss:6.930997610092163 current loss:6.911644101142883 absolute loss difference:0.019353508949279785\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Timing: train: 0.61s, val: 0.05s, epoch: 0.66s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] #progress_metric: host=algo-2, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928981.6377478, \"EndTime\": 1701928982.2988439, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 121744.0, \"count\": 1, \"min\": 121744, \"max\": 121744}, \"Total Batches Seen\": {\"sum\": 476.0, \"count\": 1, \"min\": 476, \"max\": 476}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=6575.014066988096 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:02.245] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 651, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] # Finished training epoch 29 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] Loss (name: value) total: 6.894329744226792\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] Loss (name: value) kld: 0.06577104724505368\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] Loss (name: value) recons: 6.828558725469253\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] Loss (name: value) logppx: 6.894329744226792\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=6.894329744226792\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:02.305] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 86, \"duration\": 58, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] Loss (name: value) total: 6.916692614555359\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] Loss (name: value) kld: 0.06380277127027512\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] Loss (name: value) recons: 6.852889657020569\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] Loss (name: value) logppx: 6.916692614555359\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] #validation_score (29): 6.916692614555359\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] patience losses:[6.99849534034729, 6.9979143142700195, 6.9849231243133545, 6.9890265464782715, 6.966446399688721, 6.952747225761414, 6.949137091636658, 6.949759006500244, 6.939121603965759, 6.931114912033081] min patience loss:6.931114912033081 current loss:6.916692614555359 absolute loss difference:0.014422297477722168\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] Timing: train: 0.65s, val: 0.06s, epoch: 0.72s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928981.5932798, \"EndTime\": 1701928982.3121474, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 126092.0, \"count\": 1, \"min\": 126092, \"max\": 126092}, \"Total Batches Seen\": {\"sum\": 493.0, \"count\": 1, \"min\": 493, \"max\": 493}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=6046.867978330947 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:02 INFO 140009233762112] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:02.926] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 626, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] # Finished training epoch 29 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Loss (name: value) total: 6.881007811602424\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Loss (name: value) kld: 0.06987529686268638\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Loss (name: value) recons: 6.811132487128763\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] Loss (name: value) logppx: 6.881007811602424\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:02 INFO 140221811656512] #quality_metric: host=algo-2, epoch=29, train total_loss <loss>=6.881007811602424\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:03.011] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 86, \"duration\": 84, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Loss (name: value) total: 6.90392279624939\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Loss (name: value) kld: 0.0666809119284153\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Loss (name: value) recons: 6.837241888046265\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Loss (name: value) logppx: 6.90392279624939\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] #validation_score (29): 6.90392279624939\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] patience losses:[7.005527019500732, 7.009495973587036, 6.984156370162964, 6.9618765115737915, 6.9559714794158936, 6.954965949058533, 6.936262011528015, 6.933321714401245, 6.930997610092163, 6.911644101142883] min patience loss:6.911644101142883 current loss:6.90392279624939 absolute loss difference:0.007721304893493652\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Timing: train: 0.63s, val: 0.09s, epoch: 0.72s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] #progress_metric: host=algo-2, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928982.2992253, \"EndTime\": 1701928983.0192523, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 126092.0, \"count\": 1, \"min\": 126092, \"max\": 126092}, \"Total Batches Seen\": {\"sum\": 493.0, \"count\": 1, \"min\": 493, \"max\": 493}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=6036.953560683075 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:03.705] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 679, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] # Finished training epoch 30 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Loss (name: value) total: 6.870461295632755\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Loss (name: value) kld: 0.06799928186570897\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Loss (name: value) recons: 6.802462100982666\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Loss (name: value) logppx: 6.870461295632755\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] #quality_metric: host=algo-2, epoch=30, train total_loss <loss>=6.870461295632755\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:03.799] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 89, \"duration\": 92, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Loss (name: value) total: 6.899475574493408\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Loss (name: value) kld: 0.0732404962182045\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Loss (name: value) recons: 6.826235055923462\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Loss (name: value) logppx: 6.899475574493408\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] #validation_score (30): 6.899475574493408\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] patience losses:[7.009495973587036, 6.984156370162964, 6.9618765115737915, 6.9559714794158936, 6.954965949058533, 6.936262011528015, 6.933321714401245, 6.930997610092163, 6.911644101142883, 6.90392279624939] min patience loss:6.90392279624939 current loss:6.899475574493408 absolute loss difference:0.004447221755981445\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] Timing: train: 0.69s, val: 0.10s, epoch: 0.79s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] #progress_metric: host=algo-2, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928983.0199277, \"EndTime\": 1701928983.8069632, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 130440.0, \"count\": 1, \"min\": 130440, \"max\": 130440}, \"Total Batches Seen\": {\"sum\": 510.0, \"count\": 1, \"min\": 510, \"max\": 510}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=5523.45111835988 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:03 INFO 140221811656512] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:03.019] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 706, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] # Finished training epoch 30 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) total: 6.883642925935633\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) kld: 0.0701813338433995\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) recons: 6.813461612252628\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) logppx: 6.883642925935633\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=6.883642925935633\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:03.105] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 89, \"duration\": 83, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) total: 6.907178997993469\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) kld: 0.06539475172758102\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) recons: 6.841784238815308\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) logppx: 6.907178997993469\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] #validation_score (30): 6.907178997993469\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] patience losses:[6.9979143142700195, 6.9849231243133545, 6.9890265464782715, 6.966446399688721, 6.952747225761414, 6.949137091636658, 6.949759006500244, 6.939121603965759, 6.931114912033081, 6.916692614555359] min patience loss:6.916692614555359 current loss:6.907178997993469 absolute loss difference:0.009513616561889648\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Timing: train: 0.71s, val: 0.09s, epoch: 0.80s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928982.3125916, \"EndTime\": 1701928983.1125522, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 130440.0, \"count\": 1, \"min\": 130440, \"max\": 130440}, \"Total Batches Seen\": {\"sum\": 510.0, \"count\": 1, \"min\": 510, \"max\": 510}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=5433.288780575003 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:03.807] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 694, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] # Finished training epoch 31 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) total: 6.872285197762882\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) kld: 0.06915061131996267\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) recons: 6.8031345255234665\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) logppx: 6.872285197762882\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=6.872285197762882\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:03.886] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 92, \"duration\": 77, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) total: 6.902489066123962\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) kld: 0.07735532335937023\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) recons: 6.825133800506592\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Loss (name: value) logppx: 6.902489066123962\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] #validation_score (31): 6.902489066123962\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] patience losses:[6.9849231243133545, 6.9890265464782715, 6.966446399688721, 6.952747225761414, 6.949137091636658, 6.949759006500244, 6.939121603965759, 6.931114912033081, 6.916692614555359, 6.907178997993469] min patience loss:6.907178997993469 current loss:6.902489066123962 absolute loss difference:0.004689931869506836\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] Timing: train: 0.70s, val: 0.08s, epoch: 0.78s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928983.1130662, \"EndTime\": 1701928983.892849, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 30, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 134788.0, \"count\": 1, \"min\": 134788, \"max\": 134788}, \"Total Batches Seen\": {\"sum\": 527.0, \"count\": 1, \"min\": 527, \"max\": 527}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 62.0, \"count\": 1, \"min\": 62, \"max\": 62}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=5574.449486230186 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:03 INFO 140009233762112] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:04.540] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 732, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] # Finished training epoch 31 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] Loss (name: value) total: 6.8642840104944565\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] Loss (name: value) kld: 0.07118050030925695\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] Loss (name: value) recons: 6.793103498571059\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] Loss (name: value) logppx: 6.8642840104944565\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] #quality_metric: host=algo-2, epoch=31, train total_loss <loss>=6.8642840104944565\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:04.626] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 92, \"duration\": 83, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] Loss (name: value) total: 6.894945502281189\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] Loss (name: value) kld: 0.07142582535743713\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] Loss (name: value) recons: 6.823519706726074\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] Loss (name: value) logppx: 6.894945502281189\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] #validation_score (31): 6.894945502281189\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] patience losses:[6.984156370162964, 6.9618765115737915, 6.9559714794158936, 6.954965949058533, 6.936262011528015, 6.933321714401245, 6.930997610092163, 6.911644101142883, 6.90392279624939, 6.899475574493408] min patience loss:6.899475574493408 current loss:6.894945502281189 absolute loss difference:0.004530072212219238\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] Timing: train: 0.74s, val: 0.09s, epoch: 0.82s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] #progress_metric: host=algo-2, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928983.807206, \"EndTime\": 1701928984.6321785, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 30, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 134788.0, \"count\": 1, \"min\": 134788, \"max\": 134788}, \"Total Batches Seen\": {\"sum\": 527.0, \"count\": 1, \"min\": 527, \"max\": 527}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 62.0, \"count\": 1, \"min\": 62, \"max\": 62}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=5269.309866964388 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:04 INFO 140221811656512] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:04.425] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 531, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] # Finished training epoch 32 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] Loss (name: value) total: 6.864079307107365\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] Loss (name: value) kld: 0.07143500644494505\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] Loss (name: value) recons: 6.792644220239976\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] Loss (name: value) logppx: 6.864079307107365\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=6.864079307107365\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:04.480] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 95, \"duration\": 53, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] Loss (name: value) total: 6.894657135009766\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] Loss (name: value) kld: 0.06757389381527901\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] Loss (name: value) recons: 6.827083230018616\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] Loss (name: value) logppx: 6.894657135009766\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] #validation_score (32): 6.894657135009766\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] patience losses:[6.9890265464782715, 6.966446399688721, 6.952747225761414, 6.949137091636658, 6.949759006500244, 6.939121603965759, 6.931114912033081, 6.916692614555359, 6.907178997993469, 6.902489066123962] min patience loss:6.902489066123962 current loss:6.894657135009766 absolute loss difference:0.007831931114196777\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] Timing: train: 0.53s, val: 0.06s, epoch: 0.59s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928983.8931632, \"EndTime\": 1701928984.4860897, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 31, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 139136.0, \"count\": 1, \"min\": 139136, \"max\": 139136}, \"Total Batches Seen\": {\"sum\": 544.0, \"count\": 1, \"min\": 544, \"max\": 544}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 64.0, \"count\": 1, \"min\": 64, \"max\": 64}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7331.181217340129 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:04 INFO 140009233762112] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:05.210] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 577, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] # Finished training epoch 32 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) total: 6.859537236830768\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) kld: 0.07510369972271078\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) recons: 6.784433533163631\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) logppx: 6.859537236830768\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] #quality_metric: host=algo-2, epoch=32, train total_loss <loss>=6.859537236830768\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:05.280] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 95, \"duration\": 68, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) total: 6.897757649421692\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) kld: 0.07132186181843281\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) recons: 6.826435804367065\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) logppx: 6.897757649421692\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] #validation_score (32): 6.897757649421692\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] patience losses:[6.9618765115737915, 6.9559714794158936, 6.954965949058533, 6.936262011528015, 6.933321714401245, 6.930997610092163, 6.911644101142883, 6.90392279624939, 6.899475574493408, 6.894945502281189] min patience loss:6.894945502281189 current loss:6.897757649421692 absolute loss difference:0.0028121471405029297\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Timing: train: 0.58s, val: 0.07s, epoch: 0.65s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] #progress_metric: host=algo-2, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928984.6325624, \"EndTime\": 1701928985.2822163, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 31, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 139136.0, \"count\": 1, \"min\": 139136, \"max\": 139136}, \"Total Batches Seen\": {\"sum\": 544.0, \"count\": 1, \"min\": 544, \"max\": 544}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 64.0, \"count\": 1, \"min\": 64, \"max\": 64}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=6690.901103126374 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:05.763] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 480, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] # Finished training epoch 33 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) total: 6.8531631862415985\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) kld: 0.07721358975943397\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) recons: 6.775949590346393\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) logppx: 6.8531631862415985\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] #quality_metric: host=algo-2, epoch=33, train total_loss <loss>=6.8531631862415985\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:05.829] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 98, \"duration\": 65, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) total: 6.88825249671936\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) kld: 0.08078566379845142\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) recons: 6.8074668645858765\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Loss (name: value) logppx: 6.88825249671936\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] #validation_score (33): 6.88825249671936\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] patience losses:[6.9559714794158936, 6.954965949058533, 6.936262011528015, 6.933321714401245, 6.930997610092163, 6.911644101142883, 6.90392279624939, 6.899475574493408, 6.894945502281189, 6.897757649421692] min patience loss:6.894945502281189 current loss:6.88825249671936 absolute loss difference:0.006693005561828613\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] Timing: train: 0.48s, val: 0.07s, epoch: 0.55s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] #progress_metric: host=algo-2, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928985.2825706, \"EndTime\": 1701928985.8368368, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 32, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 143484.0, \"count\": 1, \"min\": 143484, \"max\": 143484}, \"Total Batches Seen\": {\"sum\": 561.0, \"count\": 1, \"min\": 561, \"max\": 561}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 66.0, \"count\": 1, \"min\": 66, \"max\": 66}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7841.678011411106 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:05 INFO 140221811656512] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:05.012] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 525, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] # Finished training epoch 33 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) total: 6.859413146972656\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) kld: 0.07455321445184596\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) recons: 6.784859937780044\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) logppx: 6.859413146972656\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=6.859413146972656\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:05.063] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 98, \"duration\": 49, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) total: 6.900965452194214\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) kld: 0.08165029250085354\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) recons: 6.819315075874329\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) logppx: 6.900965452194214\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] #validation_score (33): 6.900965452194214\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] patience losses:[6.966446399688721, 6.952747225761414, 6.949137091636658, 6.949759006500244, 6.939121603965759, 6.931114912033081, 6.916692614555359, 6.907178997993469, 6.902489066123962, 6.894657135009766] min patience loss:6.894657135009766 current loss:6.900965452194214 absolute loss difference:0.006308317184448242\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Timing: train: 0.53s, val: 0.05s, epoch: 0.58s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928984.4864097, \"EndTime\": 1701928985.0652807, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 32, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 143484.0, \"count\": 1, \"min\": 143484, \"max\": 143484}, \"Total Batches Seen\": {\"sum\": 561.0, \"count\": 1, \"min\": 561, \"max\": 561}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 66.0, \"count\": 1, \"min\": 66, \"max\": 66}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7509.087941168599 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:05.571] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 505, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] # Finished training epoch 34 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) total: 6.855138442095588\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) kld: 0.07706274661947699\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) recons: 6.778075779185576\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) logppx: 6.855138442095588\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=6.855138442095588\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:05.629] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 101, \"duration\": 56, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) total: 6.889949798583984\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) kld: 0.07501967810094357\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) recons: 6.814930081367493\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Loss (name: value) logppx: 6.889949798583984\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] #validation_score (34): 6.889949798583984\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] patience losses:[6.952747225761414, 6.949137091636658, 6.949759006500244, 6.939121603965759, 6.931114912033081, 6.916692614555359, 6.907178997993469, 6.902489066123962, 6.894657135009766, 6.900965452194214] min patience loss:6.894657135009766 current loss:6.889949798583984 absolute loss difference:0.00470733642578125\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] Timing: train: 0.51s, val: 0.06s, epoch: 0.57s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928985.0655978, \"EndTime\": 1701928985.637236, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 33, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 147832.0, \"count\": 1, \"min\": 147832, \"max\": 147832}, \"Total Batches Seen\": {\"sum\": 578.0, \"count\": 1, \"min\": 578, \"max\": 578}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7604.16208016612 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:05 INFO 140009233762112] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:06.306] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 469, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] # Finished training epoch 34 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) total: 6.84764186073752\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) kld: 0.07749424699474783\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) recons: 6.7701475760516\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) logppx: 6.84764186073752\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] #quality_metric: host=algo-2, epoch=34, train total_loss <loss>=6.84764186073752\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:06.365] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 101, \"duration\": 57, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) total: 6.892166495323181\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) kld: 0.0814532171934843\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) recons: 6.810713291168213\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) logppx: 6.892166495323181\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] #validation_score (34): 6.892166495323181\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] patience losses:[6.954965949058533, 6.936262011528015, 6.933321714401245, 6.930997610092163, 6.911644101142883, 6.90392279624939, 6.899475574493408, 6.894945502281189, 6.897757649421692, 6.88825249671936] min patience loss:6.88825249671936 current loss:6.892166495323181 absolute loss difference:0.003913998603820801\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Timing: train: 0.47s, val: 0.06s, epoch: 0.53s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] #progress_metric: host=algo-2, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928985.837195, \"EndTime\": 1701928986.3676856, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 33, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 147832.0, \"count\": 1, \"min\": 147832, \"max\": 147832}, \"Total Batches Seen\": {\"sum\": 578.0, \"count\": 1, \"min\": 578, \"max\": 578}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=8191.455417989475 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:06.888] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 519, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] # Finished training epoch 35 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) total: 6.850825674393597\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) kld: 0.07806821605738472\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) recons: 6.7727574180154235\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) logppx: 6.850825674393597\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] #quality_metric: host=algo-2, epoch=35, train total_loss <loss>=6.850825674393597\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:06.176] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 538, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] # Finished training epoch 35 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) total: 6.851335329167983\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) kld: 0.07830152090858011\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) recons: 6.773033843320959\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) logppx: 6.851335329167983\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=6.851335329167983\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:06.224] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 104, \"duration\": 46, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) total: 6.882578492164612\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) kld: 0.07813197188079357\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) recons: 6.804446458816528\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) logppx: 6.882578492164612\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] #validation_score (35): 6.882578492164612\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] patience losses:[6.949137091636658, 6.949759006500244, 6.939121603965759, 6.931114912033081, 6.916692614555359, 6.907178997993469, 6.902489066123962, 6.894657135009766, 6.900965452194214, 6.889949798583984] min patience loss:6.889949798583984 current loss:6.882578492164612 absolute loss difference:0.007371306419372559\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Timing: train: 0.54s, val: 0.05s, epoch: 0.59s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928985.6377244, \"EndTime\": 1701928986.2316437, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 34, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 152180.0, \"count\": 1, \"min\": 152180, \"max\": 152180}, \"Total Batches Seen\": {\"sum\": 595.0, \"count\": 1, \"min\": 595, \"max\": 595}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7319.097327985398 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:06.741] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 509, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] # Finished training epoch 36 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) total: 6.852403865141027\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) kld: 0.07951840057092555\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) recons: 6.772885490866268\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) logppx: 6.852403865141027\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=6.852403865141027\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:06.794] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 107, \"duration\": 50, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) total: 6.878983497619629\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) kld: 0.07120568491518497\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) recons: 6.8077778816223145\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Loss (name: value) logppx: 6.878983497619629\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] #validation_score (36): 6.878983497619629\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] patience losses:[6.949759006500244, 6.939121603965759, 6.931114912033081, 6.916692614555359, 6.907178997993469, 6.902489066123962, 6.894657135009766, 6.900965452194214, 6.889949798583984, 6.882578492164612] min patience loss:6.882578492164612 current loss:6.878983497619629 absolute loss difference:0.00359499454498291\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] Timing: train: 0.51s, val: 0.06s, epoch: 0.57s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928986.2319171, \"EndTime\": 1701928986.8002279, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 35, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 156528.0, \"count\": 1, \"min\": 156528, \"max\": 156528}, \"Total Batches Seen\": {\"sum\": 612.0, \"count\": 1, \"min\": 612, \"max\": 612}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7649.126737500483 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:06 INFO 140009233762112] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:06.958] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 104, \"duration\": 67, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) total: 6.886139750480652\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) kld: 0.0875855777412653\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) recons: 6.798554062843323\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Loss (name: value) logppx: 6.886139750480652\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] #validation_score (35): 6.886139750480652\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] patience losses:[6.936262011528015, 6.933321714401245, 6.930997610092163, 6.911644101142883, 6.90392279624939, 6.899475574493408, 6.894945502281189, 6.897757649421692, 6.88825249671936, 6.892166495323181] min patience loss:6.88825249671936 current loss:6.886139750480652 absolute loss difference:0.002112746238708496\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] Timing: train: 0.52s, val: 0.07s, epoch: 0.59s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] #progress_metric: host=algo-2, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928986.3682797, \"EndTime\": 1701928986.9634016, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 34, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 152180.0, \"count\": 1, \"min\": 152180, \"max\": 152180}, \"Total Batches Seen\": {\"sum\": 595.0, \"count\": 1, \"min\": 595, \"max\": 595}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7303.486014439704 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:06 INFO 140221811656512] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:07.472] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 508, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] # Finished training epoch 36 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] Loss (name: value) total: 6.846912720624139\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] Loss (name: value) kld: 0.08099553471102434\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] Loss (name: value) recons: 6.765917245079489\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] Loss (name: value) logppx: 6.846912720624139\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] #quality_metric: host=algo-2, epoch=36, train total_loss <loss>=6.846912720624139\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:07.532] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 107, \"duration\": 57, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] Loss (name: value) total: 6.863357663154602\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] Loss (name: value) kld: 0.0791819877922535\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] Loss (name: value) recons: 6.784175634384155\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] Loss (name: value) logppx: 6.863357663154602\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] #validation_score (36): 6.863357663154602\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] patience losses:[6.933321714401245, 6.930997610092163, 6.911644101142883, 6.90392279624939, 6.899475574493408, 6.894945502281189, 6.897757649421692, 6.88825249671936, 6.892166495323181, 6.886139750480652] min patience loss:6.886139750480652 current loss:6.863357663154602 absolute loss difference:0.022782087326049805\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] Timing: train: 0.51s, val: 0.06s, epoch: 0.57s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] #progress_metric: host=algo-2, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928986.963765, \"EndTime\": 1701928987.5382824, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 35, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 156528.0, \"count\": 1, \"min\": 156528, \"max\": 156528}, \"Total Batches Seen\": {\"sum\": 612.0, \"count\": 1, \"min\": 612, \"max\": 612}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7565.760232455196 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:07 INFO 140221811656512] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:07.278] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 477, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] # Finished training epoch 37 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) total: 6.850324799032772\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) kld: 0.08274144854615717\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) recons: 6.76758337020874\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) logppx: 6.850324799032772\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=6.850324799032772\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:07.326] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 110, \"duration\": 45, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) total: 6.869656562805176\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) kld: 0.08111162856221199\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) recons: 6.7885448932647705\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) logppx: 6.869656562805176\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] #validation_score (37): 6.869656562805176\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] patience losses:[6.939121603965759, 6.931114912033081, 6.916692614555359, 6.907178997993469, 6.902489066123962, 6.894657135009766, 6.900965452194214, 6.889949798583984, 6.882578492164612, 6.878983497619629] min patience loss:6.878983497619629 current loss:6.869656562805176 absolute loss difference:0.009326934814453125\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Timing: train: 0.48s, val: 0.05s, epoch: 0.53s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] #progress_metric: host=algo-1, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928986.8005073, \"EndTime\": 1701928987.332217, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 36, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 160876.0, \"count\": 1, \"min\": 160876, \"max\": 160876}, \"Total Batches Seen\": {\"sum\": 629.0, \"count\": 1, \"min\": 629, \"max\": 629}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 74.0, \"count\": 1, \"min\": 74, \"max\": 74}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8175.367783727566 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:07.828] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 495, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] # Finished training epoch 38 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) total: 6.84714693181655\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) kld: 0.07990445766378851\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) recons: 6.767242487739114\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) logppx: 6.84714693181655\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=6.84714693181655\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:07.878] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 113, \"duration\": 47, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) total: 6.8666582107543945\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) kld: 0.07723608613014221\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) recons: 6.789422273635864\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Loss (name: value) logppx: 6.8666582107543945\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] #validation_score (38): 6.8666582107543945\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] patience losses:[6.931114912033081, 6.916692614555359, 6.907178997993469, 6.902489066123962, 6.894657135009766, 6.900965452194214, 6.889949798583984, 6.882578492164612, 6.878983497619629, 6.869656562805176] min patience loss:6.869656562805176 current loss:6.8666582107543945 absolute loss difference:0.00299835205078125\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] Timing: train: 0.50s, val: 0.05s, epoch: 0.55s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928987.3325129, \"EndTime\": 1701928987.8843, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 37, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 165224.0, \"count\": 1, \"min\": 165224, \"max\": 165224}, \"Total Batches Seen\": {\"sum\": 646.0, \"count\": 1, \"min\": 646, \"max\": 646}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 76.0, \"count\": 1, \"min\": 76, \"max\": 76}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7875.321088192136 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:07 INFO 140009233762112] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:08.041] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 502, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] # Finished training epoch 37 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) total: 6.84462376201854\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) kld: 0.08252515468527288\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) recons: 6.762098592870376\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) logppx: 6.84462376201854\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] #quality_metric: host=algo-2, epoch=37, train total_loss <loss>=6.84462376201854\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:08.090] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 110, \"duration\": 48, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) total: 6.890353679656982\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) kld: 0.07288140058517456\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) recons: 6.817472338676453\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) logppx: 6.890353679656982\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] #validation_score (37): 6.890353679656982\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] patience losses:[6.930997610092163, 6.911644101142883, 6.90392279624939, 6.899475574493408, 6.894945502281189, 6.897757649421692, 6.88825249671936, 6.892166495323181, 6.886139750480652, 6.863357663154602] min patience loss:6.863357663154602 current loss:6.890353679656982 absolute loss difference:0.02699601650238037\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Timing: train: 0.50s, val: 0.05s, epoch: 0.55s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] #progress_metric: host=algo-2, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928987.5387228, \"EndTime\": 1701928988.0925245, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 36, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 160876.0, \"count\": 1, \"min\": 160876, \"max\": 160876}, \"Total Batches Seen\": {\"sum\": 629.0, \"count\": 1, \"min\": 629, \"max\": 629}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 74.0, \"count\": 1, \"min\": 74, \"max\": 74}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7848.657208483014 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:08.590] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 497, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] # Finished training epoch 38 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) total: 6.840752377229578\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) kld: 0.08087066823945326\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) recons: 6.7598817769218895\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) logppx: 6.840752377229578\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] #quality_metric: host=algo-2, epoch=38, train total_loss <loss>=6.840752377229578\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:08.655] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 113, \"duration\": 63, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) total: 6.876943230628967\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) kld: 0.09196827560663223\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) recons: 6.784974932670593\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Loss (name: value) logppx: 6.876943230628967\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] #validation_score (38): 6.876943230628967\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] patience losses:[6.911644101142883, 6.90392279624939, 6.899475574493408, 6.894945502281189, 6.897757649421692, 6.88825249671936, 6.892166495323181, 6.886139750480652, 6.863357663154602, 6.890353679656982] min patience loss:6.863357663154602 current loss:6.876943230628967 absolute loss difference:0.013585567474365234\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] Timing: train: 0.50s, val: 0.07s, epoch: 0.56s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] #progress_metric: host=algo-2, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928988.0928295, \"EndTime\": 1701928988.6569707, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 37, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 165224.0, \"count\": 1, \"min\": 165224, \"max\": 165224}, \"Total Batches Seen\": {\"sum\": 646.0, \"count\": 1, \"min\": 646, \"max\": 646}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 76.0, \"count\": 1, \"min\": 76, \"max\": 76}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7705.045692009148 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:08 INFO 140221811656512] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:08.413] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 528, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] # Finished training epoch 39 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Loss (name: value) total: 6.843621394213508\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Loss (name: value) kld: 0.08265532816157621\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Loss (name: value) recons: 6.760966048521154\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Loss (name: value) logppx: 6.843621394213508\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] #quality_metric: host=algo-1, epoch=39, train total_loss <loss>=6.843621394213508\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:08.464] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 116, \"duration\": 48, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Loss (name: value) total: 6.87684166431427\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Loss (name: value) kld: 0.06881611421704292\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Loss (name: value) recons: 6.8080257177352905\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Loss (name: value) logppx: 6.87684166431427\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] #validation_score (39): 6.87684166431427\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] patience losses:[6.916692614555359, 6.907178997993469, 6.902489066123962, 6.894657135009766, 6.900965452194214, 6.889949798583984, 6.882578492164612, 6.878983497619629, 6.869656562805176, 6.8666582107543945] min patience loss:6.8666582107543945 current loss:6.87684166431427 absolute loss difference:0.010183453559875488\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Timing: train: 0.53s, val: 0.05s, epoch: 0.58s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] #progress_metric: host=algo-1, completed 78.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928987.8847878, \"EndTime\": 1701928988.4666936, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 38, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 169572.0, \"count\": 1, \"min\": 169572, \"max\": 169572}, \"Total Batches Seen\": {\"sum\": 663.0, \"count\": 1, \"min\": 663, \"max\": 663}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 78.0, \"count\": 1, \"min\": 78, \"max\": 78}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7469.8293036083 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:09.158] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 500, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] # Finished training epoch 39 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) total: 6.833966227138744\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) kld: 0.08212928999872769\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) recons: 6.751836832831888\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) logppx: 6.833966227138744\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] #quality_metric: host=algo-2, epoch=39, train total_loss <loss>=6.833966227138744\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:09.208] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 116, \"duration\": 48, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) total: 6.867466688156128\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) kld: 0.08116097003221512\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) recons: 6.786305785179138\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) logppx: 6.867466688156128\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] #validation_score (39): 6.867466688156128\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] patience losses:[6.90392279624939, 6.899475574493408, 6.894945502281189, 6.897757649421692, 6.88825249671936, 6.892166495323181, 6.886139750480652, 6.863357663154602, 6.890353679656982, 6.876943230628967] min patience loss:6.863357663154602 current loss:6.867466688156128 absolute loss difference:0.004109025001525879\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Timing: train: 0.50s, val: 0.05s, epoch: 0.55s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] #progress_metric: host=algo-2, completed 78.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928988.6572955, \"EndTime\": 1701928989.2102537, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 38, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 169572.0, \"count\": 1, \"min\": 169572, \"max\": 169572}, \"Total Batches Seen\": {\"sum\": 663.0, \"count\": 1, \"min\": 663, \"max\": 663}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 78.0, \"count\": 1, \"min\": 78, \"max\": 78}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7860.558529303121 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:09.681] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 470, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] # Finished training epoch 40 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) total: 6.830035406000474\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) kld: 0.0860375884701224\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) recons: 6.743997854344985\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) logppx: 6.830035406000474\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] #quality_metric: host=algo-2, epoch=40, train total_loss <loss>=6.830035406000474\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:09.752] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 119, \"duration\": 65, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) total: 6.855812907218933\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) kld: 0.08239057846367359\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) recons: 6.773422360420227\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Loss (name: value) logppx: 6.855812907218933\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] #validation_score (40): 6.855812907218933\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] patience losses:[6.899475574493408, 6.894945502281189, 6.897757649421692, 6.88825249671936, 6.892166495323181, 6.886139750480652, 6.863357663154602, 6.890353679656982, 6.876943230628967, 6.867466688156128] min patience loss:6.863357663154602 current loss:6.855812907218933 absolute loss difference:0.007544755935668945\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] Timing: train: 0.47s, val: 0.07s, epoch: 0.55s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] #progress_metric: host=algo-2, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928989.2106092, \"EndTime\": 1701928989.757871, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 39, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 173920.0, \"count\": 1, \"min\": 173920, \"max\": 173920}, \"Total Batches Seen\": {\"sum\": 680.0, \"count\": 1, \"min\": 680, \"max\": 680}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7942.283799544459 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:09 INFO 140221811656512] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:08.985] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 518, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] # Finished training epoch 40 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Loss (name: value) total: 6.839620309717515\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Loss (name: value) kld: 0.08360433885279824\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Loss (name: value) recons: 6.756016001981847\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] Loss (name: value) logppx: 6.839620309717515\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:08 INFO 140009233762112] #quality_metric: host=algo-1, epoch=40, train total_loss <loss>=6.839620309717515\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:09.055] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 119, \"duration\": 68, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Loss (name: value) total: 6.8612223863601685\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Loss (name: value) kld: 0.08018372394144535\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Loss (name: value) recons: 6.781038761138916\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Loss (name: value) logppx: 6.8612223863601685\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] #validation_score (40): 6.8612223863601685\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] patience losses:[6.907178997993469, 6.902489066123962, 6.894657135009766, 6.900965452194214, 6.889949798583984, 6.882578492164612, 6.878983497619629, 6.869656562805176, 6.8666582107543945, 6.87684166431427] min patience loss:6.8666582107543945 current loss:6.8612223863601685 absolute loss difference:0.005435824394226074\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Timing: train: 0.52s, val: 0.07s, epoch: 0.59s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928988.466999, \"EndTime\": 1701928989.0604537, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 39, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 173920.0, \"count\": 1, \"min\": 173920, \"max\": 173920}, \"Total Batches Seen\": {\"sum\": 680.0, \"count\": 1, \"min\": 680, \"max\": 680}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7324.353201145432 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:09.563] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 502, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] # Finished training epoch 41 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Loss (name: value) total: 6.832963550792021\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Loss (name: value) kld: 0.08290461669949924\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Loss (name: value) recons: 6.750058931462905\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Loss (name: value) logppx: 6.832963550792021\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] #quality_metric: host=algo-1, epoch=41, train total_loss <loss>=6.832963550792021\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:09.631] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 122, \"duration\": 65, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Loss (name: value) total: 6.877000331878662\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Loss (name: value) kld: 0.09288059547543526\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Loss (name: value) recons: 6.7841198444366455\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Loss (name: value) logppx: 6.877000331878662\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] #validation_score (41): 6.877000331878662\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] patience losses:[6.902489066123962, 6.894657135009766, 6.900965452194214, 6.889949798583984, 6.882578492164612, 6.878983497619629, 6.869656562805176, 6.8666582107543945, 6.87684166431427, 6.8612223863601685] min patience loss:6.8612223863601685 current loss:6.877000331878662 absolute loss difference:0.015777945518493652\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] Timing: train: 0.50s, val: 0.07s, epoch: 0.57s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] #progress_metric: host=algo-1, completed 82.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928989.0608492, \"EndTime\": 1701928989.6335893, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 40, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 178268.0, \"count\": 1, \"min\": 178268, \"max\": 178268}, \"Total Batches Seen\": {\"sum\": 697.0, \"count\": 1, \"min\": 697, \"max\": 697}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 82.0, \"count\": 1, \"min\": 82, \"max\": 82}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7589.465870189279 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:09 INFO 140009233762112] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:10.262] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 503, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] # Finished training epoch 41 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) total: 6.825205017538631\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) kld: 0.08490576112971586\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) recons: 6.74029925290276\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) logppx: 6.825205017538631\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] #quality_metric: host=algo-2, epoch=41, train total_loss <loss>=6.825205017538631\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:10.325] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 122, \"duration\": 61, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) total: 6.860332608222961\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) kld: 0.09159804508090019\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) recons: 6.768734455108643\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) logppx: 6.860332608222961\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] #validation_score (41): 6.860332608222961\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] patience losses:[6.894945502281189, 6.897757649421692, 6.88825249671936, 6.892166495323181, 6.886139750480652, 6.863357663154602, 6.890353679656982, 6.876943230628967, 6.867466688156128, 6.855812907218933] min patience loss:6.855812907218933 current loss:6.860332608222961 absolute loss difference:0.00451970100402832\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Timing: train: 0.51s, val: 0.06s, epoch: 0.57s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] #progress_metric: host=algo-2, completed 82.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928989.758233, \"EndTime\": 1701928990.3274977, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 40, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 178268.0, \"count\": 1, \"min\": 178268, \"max\": 178268}, \"Total Batches Seen\": {\"sum\": 697.0, \"count\": 1, \"min\": 697, \"max\": 697}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 82.0, \"count\": 1, \"min\": 82, \"max\": 82}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7635.713959369609 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:10.839] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 511, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] # Finished training epoch 42 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) total: 6.8202027152566345\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) kld: 0.08583276061450734\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) recons: 6.734369895037482\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) logppx: 6.8202027152566345\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] #quality_metric: host=algo-2, epoch=42, train total_loss <loss>=6.8202027152566345\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:10.889] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 125, \"duration\": 47, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) total: 6.864179015159607\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) kld: 0.0785624086856842\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) recons: 6.785616755485535\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Loss (name: value) logppx: 6.864179015159607\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] #validation_score (42): 6.864179015159607\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] patience losses:[6.897757649421692, 6.88825249671936, 6.892166495323181, 6.886139750480652, 6.863357663154602, 6.890353679656982, 6.876943230628967, 6.867466688156128, 6.855812907218933, 6.860332608222961] min patience loss:6.855812907218933 current loss:6.864179015159607 absolute loss difference:0.008366107940673828\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] Timing: train: 0.51s, val: 0.05s, epoch: 0.56s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] #progress_metric: host=algo-2, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928990.3278775, \"EndTime\": 1701928990.891541, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 41, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 182616.0, \"count\": 1, \"min\": 182616, \"max\": 182616}, \"Total Batches Seen\": {\"sum\": 714.0, \"count\": 1, \"min\": 714, \"max\": 714}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 84.0, \"count\": 1, \"min\": 84, \"max\": 84}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7711.232612675848 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:10 INFO 140221811656512] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:10.105] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 470, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] # Finished training epoch 42 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) total: 6.82689411499921\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) kld: 0.08477210647919599\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) recons: 6.7421220330631035\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) logppx: 6.82689411499921\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] #quality_metric: host=algo-1, epoch=42, train total_loss <loss>=6.82689411499921\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:10.152] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 125, \"duration\": 45, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) total: 6.853747129440308\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) kld: 0.08090694807469845\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) recons: 6.772840142250061\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) logppx: 6.853747129440308\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] #validation_score (42): 6.853747129440308\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] patience losses:[6.894657135009766, 6.900965452194214, 6.889949798583984, 6.882578492164612, 6.878983497619629, 6.869656562805176, 6.8666582107543945, 6.87684166431427, 6.8612223863601685, 6.877000331878662] min patience loss:6.8612223863601685 current loss:6.853747129440308 absolute loss difference:0.00747525691986084\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Timing: train: 0.47s, val: 0.05s, epoch: 0.52s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] #progress_metric: host=algo-1, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928989.633943, \"EndTime\": 1701928990.1584735, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 41, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 182616.0, \"count\": 1, \"min\": 182616, \"max\": 182616}, \"Total Batches Seen\": {\"sum\": 714.0, \"count\": 1, \"min\": 714, \"max\": 714}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 84.0, \"count\": 1, \"min\": 84, \"max\": 84}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8286.968747614357 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:10.689] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 530, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] # Finished training epoch 43 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) total: 6.820954126470229\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) kld: 0.08790653900188558\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) recons: 6.733047597548541\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) logppx: 6.820954126470229\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] #quality_metric: host=algo-1, epoch=43, train total_loss <loss>=6.820954126470229\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:10.757] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 128, \"duration\": 66, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) total: 6.868358850479126\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) kld: 0.09651706553995609\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) recons: 6.771841764450073\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Loss (name: value) logppx: 6.868358850479126\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] #validation_score (43): 6.868358850479126\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] patience losses:[6.900965452194214, 6.889949798583984, 6.882578492164612, 6.878983497619629, 6.869656562805176, 6.8666582107543945, 6.87684166431427, 6.8612223863601685, 6.877000331878662, 6.853747129440308] min patience loss:6.853747129440308 current loss:6.868358850479126 absolute loss difference:0.01461172103881836\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] Timing: train: 0.53s, val: 0.07s, epoch: 0.60s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] #progress_metric: host=algo-1, completed 86.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928990.158812, \"EndTime\": 1701928990.7594454, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 42, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 186964.0, \"count\": 1, \"min\": 186964, \"max\": 186964}, \"Total Batches Seen\": {\"sum\": 731.0, \"count\": 1, \"min\": 731, \"max\": 731}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 86.0, \"count\": 1, \"min\": 86, \"max\": 86}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7237.120249914481 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:10 INFO 140009233762112] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:11.379] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 487, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] # Finished training epoch 43 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Loss (name: value) total: 6.820137304418227\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Loss (name: value) kld: 0.0889340967816465\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Loss (name: value) recons: 6.731203191420612\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Loss (name: value) logppx: 6.820137304418227\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] #quality_metric: host=algo-2, epoch=43, train total_loss <loss>=6.820137304418227\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:11.432] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 128, \"duration\": 51, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Loss (name: value) total: 6.861696481704712\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Loss (name: value) kld: 0.08738179691135883\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Loss (name: value) recons: 6.774314761161804\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Loss (name: value) logppx: 6.861696481704712\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] #validation_score (43): 6.861696481704712\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] patience losses:[6.88825249671936, 6.892166495323181, 6.886139750480652, 6.863357663154602, 6.890353679656982, 6.876943230628967, 6.867466688156128, 6.855812907218933, 6.860332608222961, 6.864179015159607] min patience loss:6.855812907218933 current loss:6.861696481704712 absolute loss difference:0.005883574485778809\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Timing: train: 0.49s, val: 0.05s, epoch: 0.54s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] #progress_metric: host=algo-2, completed 86.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928990.8919346, \"EndTime\": 1701928991.4340844, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 42, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 186964.0, \"count\": 1, \"min\": 186964, \"max\": 186964}, \"Total Batches Seen\": {\"sum\": 731.0, \"count\": 1, \"min\": 731, \"max\": 731}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 86.0, \"count\": 1, \"min\": 86, \"max\": 86}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=8017.526344828435 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:11.259] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 499, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] # Finished training epoch 44 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) total: 6.825138793272131\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) kld: 0.08713928829221164\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) recons: 6.737999579485725\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) logppx: 6.825138793272131\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] #quality_metric: host=algo-1, epoch=44, train total_loss <loss>=6.825138793272131\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:11.318] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 131, \"duration\": 57, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) total: 6.858085513114929\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) kld: 0.09510372206568718\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) recons: 6.76298189163208\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) logppx: 6.858085513114929\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] #validation_score (44): 6.858085513114929\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] patience losses:[6.889949798583984, 6.882578492164612, 6.878983497619629, 6.869656562805176, 6.8666582107543945, 6.87684166431427, 6.8612223863601685, 6.877000331878662, 6.853747129440308, 6.868358850479126] min patience loss:6.853747129440308 current loss:6.858085513114929 absolute loss difference:0.004338383674621582\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Timing: train: 0.50s, val: 0.06s, epoch: 0.56s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] #progress_metric: host=algo-1, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928990.7598286, \"EndTime\": 1701928991.320506, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 43, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 191312.0, \"count\": 1, \"min\": 191312, \"max\": 191312}, \"Total Batches Seen\": {\"sum\": 748.0, \"count\": 1, \"min\": 748, \"max\": 748}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 88.0, \"count\": 1, \"min\": 88, \"max\": 88}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7751.8936956045845 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:11.827] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 504, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] # Finished training epoch 45 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) total: 6.825904341305003\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) kld: 0.08784741864484899\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) recons: 6.7380569401909325\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) logppx: 6.825904341305003\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] #quality_metric: host=algo-1, epoch=45, train total_loss <loss>=6.825904341305003\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:11.876] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 134, \"duration\": 46, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) total: 6.855632781982422\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) kld: 0.08687608316540718\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) recons: 6.768756628036499\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Loss (name: value) logppx: 6.855632781982422\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] #validation_score (45): 6.855632781982422\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] patience losses:[6.882578492164612, 6.878983497619629, 6.869656562805176, 6.8666582107543945, 6.87684166431427, 6.8612223863601685, 6.877000331878662, 6.853747129440308, 6.868358850479126, 6.858085513114929] min patience loss:6.853747129440308 current loss:6.855632781982422 absolute loss difference:0.0018856525421142578\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] Timing: train: 0.51s, val: 0.05s, epoch: 0.56s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928991.32091, \"EndTime\": 1701928991.878236, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 44, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 195660.0, \"count\": 1, \"min\": 195660, \"max\": 195660}, \"Total Batches Seen\": {\"sum\": 765.0, \"count\": 1, \"min\": 765, \"max\": 765}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 90.0, \"count\": 1, \"min\": 90, \"max\": 90}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7799.374911846203 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:11 INFO 140009233762112] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:11.951] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 516, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] # Finished training epoch 44 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Loss (name: value) total: 6.8192417481366325\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Loss (name: value) kld: 0.08880248578155742\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Loss (name: value) recons: 6.730439242194681\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] Loss (name: value) logppx: 6.8192417481366325\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:11 INFO 140221811656512] #quality_metric: host=algo-2, epoch=44, train total_loss <loss>=6.8192417481366325\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:12.004] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 131, \"duration\": 52, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Loss (name: value) total: 6.846298456192017\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Loss (name: value) kld: 0.09596460498869419\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Loss (name: value) recons: 6.750333786010742\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Loss (name: value) logppx: 6.846298456192017\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] #validation_score (44): 6.846298456192017\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] patience losses:[6.892166495323181, 6.886139750480652, 6.863357663154602, 6.890353679656982, 6.876943230628967, 6.867466688156128, 6.855812907218933, 6.860332608222961, 6.864179015159607, 6.861696481704712] min patience loss:6.855812907218933 current loss:6.846298456192017 absolute loss difference:0.009514451026916504\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Timing: train: 0.52s, val: 0.06s, epoch: 0.58s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] #progress_metric: host=algo-2, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928991.434397, \"EndTime\": 1701928992.01025, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 43, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 191312.0, \"count\": 1, \"min\": 191312, \"max\": 191312}, \"Total Batches Seen\": {\"sum\": 748.0, \"count\": 1, \"min\": 748, \"max\": 748}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 88.0, \"count\": 1, \"min\": 88, \"max\": 88}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7547.658574701683 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:12.467] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 456, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] # Finished training epoch 45 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Loss (name: value) total: 6.81238738228293\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Loss (name: value) kld: 0.087652247179957\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Loss (name: value) recons: 6.724735147812787\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Loss (name: value) logppx: 6.81238738228293\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] #quality_metric: host=algo-2, epoch=45, train total_loss <loss>=6.81238738228293\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:12.518] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 134, \"duration\": 49, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Loss (name: value) total: 6.847935438156128\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Loss (name: value) kld: 0.09538395516574383\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Loss (name: value) recons: 6.752551555633545\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Loss (name: value) logppx: 6.847935438156128\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] #validation_score (45): 6.847935438156128\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] patience losses:[6.886139750480652, 6.863357663154602, 6.890353679656982, 6.876943230628967, 6.867466688156128, 6.855812907218933, 6.860332608222961, 6.864179015159607, 6.861696481704712, 6.846298456192017] min patience loss:6.846298456192017 current loss:6.847935438156128 absolute loss difference:0.0016369819641113281\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] Timing: train: 0.46s, val: 0.05s, epoch: 0.51s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] #progress_metric: host=algo-2, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928992.010619, \"EndTime\": 1701928992.5202506, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 44, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 195660.0, \"count\": 1, \"min\": 195660, \"max\": 195660}, \"Total Batches Seen\": {\"sum\": 765.0, \"count\": 1, \"min\": 765, \"max\": 765}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 90.0, \"count\": 1, \"min\": 90, \"max\": 90}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=8528.915326465367 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:12 INFO 140221811656512] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:12.342] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 463, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] # Finished training epoch 46 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) total: 6.815215980305391\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) kld: 0.09037852988523595\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) recons: 6.7248374153586\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) logppx: 6.815215980305391\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] #quality_metric: host=algo-1, epoch=46, train total_loss <loss>=6.815215980305391\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:12.411] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 137, \"duration\": 67, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) total: 6.853926062583923\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) kld: 0.08296193368732929\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) recons: 6.7709641456604\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) logppx: 6.853926062583923\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] #validation_score (46): 6.853926062583923\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] patience losses:[6.878983497619629, 6.869656562805176, 6.8666582107543945, 6.87684166431427, 6.8612223863601685, 6.877000331878662, 6.853747129440308, 6.868358850479126, 6.858085513114929, 6.855632781982422] min patience loss:6.853747129440308 current loss:6.853926062583923 absolute loss difference:0.00017893314361572266\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Timing: train: 0.47s, val: 0.07s, epoch: 0.54s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] #progress_metric: host=algo-1, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928991.878551, \"EndTime\": 1701928992.4139736, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 45, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 200008.0, \"count\": 1, \"min\": 200008, \"max\": 200008}, \"Total Batches Seen\": {\"sum\": 782.0, \"count\": 1, \"min\": 782, \"max\": 782}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8118.324553425758 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:13.028] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 507, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] # Finished training epoch 46 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) total: 6.806739498587215\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) kld: 0.08969150220646578\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) recons: 6.717048055985394\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) logppx: 6.806739498587215\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] #quality_metric: host=algo-2, epoch=46, train total_loss <loss>=6.806739498587215\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:13.084] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 137, \"duration\": 54, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) total: 6.859824061393738\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) kld: 0.08576651476323605\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) recons: 6.774057507514954\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) logppx: 6.859824061393738\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] #validation_score (46): 6.859824061393738\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] patience losses:[6.863357663154602, 6.890353679656982, 6.876943230628967, 6.867466688156128, 6.855812907218933, 6.860332608222961, 6.864179015159607, 6.861696481704712, 6.846298456192017, 6.847935438156128] min patience loss:6.846298456192017 current loss:6.859824061393738 absolute loss difference:0.013525605201721191\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Timing: train: 0.51s, val: 0.06s, epoch: 0.57s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] #progress_metric: host=algo-2, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928992.5206163, \"EndTime\": 1701928993.0862095, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 45, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 200008.0, \"count\": 1, \"min\": 200008, \"max\": 200008}, \"Total Batches Seen\": {\"sum\": 782.0, \"count\": 1, \"min\": 782, \"max\": 782}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7684.8503330469275 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:13.577] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 490, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] # Finished training epoch 47 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) total: 6.810729784124038\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) kld: 0.09276732276467715\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) recons: 6.7179624613593605\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) logppx: 6.810729784124038\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] #quality_metric: host=algo-2, epoch=47, train total_loss <loss>=6.810729784124038\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:13.628] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 140, \"duration\": 50, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) total: 6.842593550682068\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) kld: 0.09331592544913292\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) recons: 6.749277710914612\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Loss (name: value) logppx: 6.842593550682068\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] #validation_score (47): 6.842593550682068\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] patience losses:[6.890353679656982, 6.876943230628967, 6.867466688156128, 6.855812907218933, 6.860332608222961, 6.864179015159607, 6.861696481704712, 6.846298456192017, 6.847935438156128, 6.859824061393738] min patience loss:6.846298456192017 current loss:6.842593550682068 absolute loss difference:0.0037049055099487305\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] Timing: train: 0.49s, val: 0.06s, epoch: 0.55s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] #progress_metric: host=algo-2, completed 94.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928993.086572, \"EndTime\": 1701928993.637449, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 46, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 204356.0, \"count\": 1, \"min\": 204356, \"max\": 204356}, \"Total Batches Seen\": {\"sum\": 799.0, \"count\": 1, \"min\": 799, \"max\": 799}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 94.0, \"count\": 1, \"min\": 94, \"max\": 94}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7890.456684082913 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:13 INFO 140221811656512] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:12.933] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 519, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] # Finished training epoch 47 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) total: 6.815208294812371\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) kld: 0.0902566844049622\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) recons: 6.7249515757841225\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) logppx: 6.815208294812371\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] #quality_metric: host=algo-1, epoch=47, train total_loss <loss>=6.815208294812371\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:12.986] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 140, \"duration\": 50, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) total: 6.836687088012695\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) kld: 0.08771075494587421\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) recons: 6.748976111412048\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Loss (name: value) logppx: 6.836687088012695\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] #validation_score (47): 6.836687088012695\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] patience losses:[6.869656562805176, 6.8666582107543945, 6.87684166431427, 6.8612223863601685, 6.877000331878662, 6.853747129440308, 6.868358850479126, 6.858085513114929, 6.855632781982422, 6.853926062583923] min patience loss:6.853747129440308 current loss:6.836687088012695 absolute loss difference:0.017060041427612305\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] Timing: train: 0.52s, val: 0.06s, epoch: 0.58s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] #progress_metric: host=algo-1, completed 94.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928992.4142716, \"EndTime\": 1701928992.9921813, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 46, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 204356.0, \"count\": 1, \"min\": 204356, \"max\": 204356}, \"Total Batches Seen\": {\"sum\": 799.0, \"count\": 1, \"min\": 799, \"max\": 799}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 94.0, \"count\": 1, \"min\": 94, \"max\": 94}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7521.655856671081 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:12 INFO 140009233762112] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:13.477] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 484, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] # Finished training epoch 48 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Loss (name: value) total: 6.8115825372583725\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Loss (name: value) kld: 0.0888998385737924\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Loss (name: value) recons: 6.72268258824068\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Loss (name: value) logppx: 6.8115825372583725\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] #quality_metric: host=algo-1, epoch=48, train total_loss <loss>=6.8115825372583725\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:13.549] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 143, \"duration\": 69, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Loss (name: value) total: 6.848626255989075\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Loss (name: value) kld: 0.09973837435245514\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Loss (name: value) recons: 6.748887896537781\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Loss (name: value) logppx: 6.848626255989075\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] #validation_score (48): 6.848626255989075\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] patience losses:[6.8666582107543945, 6.87684166431427, 6.8612223863601685, 6.877000331878662, 6.853747129440308, 6.868358850479126, 6.858085513114929, 6.855632781982422, 6.853926062583923, 6.836687088012695] min patience loss:6.836687088012695 current loss:6.848626255989075 absolute loss difference:0.011939167976379395\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] Timing: train: 0.49s, val: 0.07s, epoch: 0.56s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] #progress_metric: host=algo-1, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928992.9924874, \"EndTime\": 1701928993.551403, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 47, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 208704.0, \"count\": 1, \"min\": 208704, \"max\": 208704}, \"Total Batches Seen\": {\"sum\": 816.0, \"count\": 1, \"min\": 816, \"max\": 816}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 96.0, \"count\": 1, \"min\": 96, \"max\": 96}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=7775.654646419167 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:13 INFO 140009233762112] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:14.113] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 475, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] # Finished training epoch 48 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) total: 6.803159517400405\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) kld: 0.0915812166298137\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) recons: 6.711578256943646\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) logppx: 6.803159517400405\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] #quality_metric: host=algo-2, epoch=48, train total_loss <loss>=6.803159517400405\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:14.174] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 143, \"duration\": 58, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) total: 6.84159779548645\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) kld: 0.09801171161234379\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) recons: 6.74358606338501\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) logppx: 6.84159779548645\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] #validation_score (48): 6.84159779548645\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] patience losses:[6.876943230628967, 6.867466688156128, 6.855812907218933, 6.860332608222961, 6.864179015159607, 6.861696481704712, 6.846298456192017, 6.847935438156128, 6.859824061393738, 6.842593550682068] min patience loss:6.842593550682068 current loss:6.84159779548645 absolute loss difference:0.0009957551956176758\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Timing: train: 0.48s, val: 0.07s, epoch: 0.54s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] #progress_metric: host=algo-2, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928993.6377566, \"EndTime\": 1701928994.181408, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 47, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 208704.0, \"count\": 1, \"min\": 208704, \"max\": 208704}, \"Total Batches Seen\": {\"sum\": 816.0, \"count\": 1, \"min\": 816, \"max\": 816}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 96.0, \"count\": 1, \"min\": 96, \"max\": 96}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7995.304482400481 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:14.724] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 542, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] # Finished training epoch 49 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) total: 6.800534584942986\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) kld: 0.09288908190586988\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) recons: 6.707645528456744\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) logppx: 6.800534584942986\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] #quality_metric: host=algo-2, epoch=49, train total_loss <loss>=6.800534584942986\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:14.777] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 146, \"duration\": 51, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) total: 6.835043430328369\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) kld: 0.09277732111513615\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) recons: 6.7422661781311035\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Loss (name: value) logppx: 6.835043430328369\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] #validation_score (49): 6.835043430328369\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] patience losses:[6.867466688156128, 6.855812907218933, 6.860332608222961, 6.864179015159607, 6.861696481704712, 6.846298456192017, 6.847935438156128, 6.859824061393738, 6.842593550682068, 6.84159779548645] min patience loss:6.84159779548645 current loss:6.835043430328369 absolute loss difference:0.006554365158081055\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] Timing: train: 0.54s, val: 0.06s, epoch: 0.60s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] #progress_metric: host=algo-2, completed 98.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928994.1817198, \"EndTime\": 1701928994.7829316, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 48, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 213052.0, \"count\": 1, \"min\": 213052, \"max\": 213052}, \"Total Batches Seen\": {\"sum\": 833.0, \"count\": 1, \"min\": 833, \"max\": 833}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 98.0, \"count\": 1, \"min\": 98, \"max\": 98}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=7230.096454496798 records/second\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] \u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:14 INFO 140221811656512] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:14.039] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 487, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] # Finished training epoch 49 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) total: 6.805734381956213\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) kld: 0.091464683851775\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) recons: 6.7142697222092576\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) logppx: 6.805734381956213\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] #quality_metric: host=algo-1, epoch=49, train total_loss <loss>=6.805734381956213\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:14.093] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 146, \"duration\": 51, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) total: 6.842121481895447\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) kld: 0.09468234702944756\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) recons: 6.747439026832581\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) logppx: 6.842121481895447\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] #validation_score (49): 6.842121481895447\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] patience losses:[6.87684166431427, 6.8612223863601685, 6.877000331878662, 6.853747129440308, 6.868358850479126, 6.858085513114929, 6.855632781982422, 6.853926062583923, 6.836687088012695, 6.848626255989075] min patience loss:6.836687088012695 current loss:6.842121481895447 absolute loss difference:0.005434393882751465\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Timing: train: 0.49s, val: 0.05s, epoch: 0.54s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] #progress_metric: host=algo-1, completed 98.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928993.5518556, \"EndTime\": 1701928994.0946712, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 48, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 213052.0, \"count\": 1, \"min\": 213052, \"max\": 213052}, \"Total Batches Seen\": {\"sum\": 833.0, \"count\": 1, \"min\": 833, \"max\": 833}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 98.0, \"count\": 1, \"min\": 98, \"max\": 98}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8007.714819654625 records/second\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] \u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:14.588] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 493, \"num_examples\": 17, \"num_bytes\": 1705620}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] # Finished training epoch 50 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) total: 6.807157825021183\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) kld: 0.09410261538098841\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) recons: 6.7130551899180695\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) logppx: 6.807157825021183\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] #quality_metric: host=algo-1, epoch=50, train total_loss <loss>=6.807157825021183\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:14.635] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 149, \"duration\": 45, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) total: 6.849194288253784\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) kld: 0.09405626356601715\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) recons: 6.755138039588928\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Loss (name: value) logppx: 6.849194288253784\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] #validation_score (50): 6.849194288253784\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] patience losses:[6.8612223863601685, 6.877000331878662, 6.853747129440308, 6.868358850479126, 6.858085513114929, 6.855632781982422, 6.853926062583923, 6.836687088012695, 6.848626255989075, 6.842121481895447] min patience loss:6.836687088012695 current loss:6.849194288253784 absolute loss difference:0.012507200241088867\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] Timing: train: 0.49s, val: 0.05s, epoch: 0.54s\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928994.0949707, \"EndTime\": 1701928994.636483, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 49, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 217400.0, \"count\": 1, \"min\": 217400, \"max\": 217400}, \"Total Batches Seen\": {\"sum\": 850.0, \"count\": 1, \"min\": 850, \"max\": 850}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:14 INFO 140009233762112] #throughput_metric: host=algo-1, train throughput=8027.026281191005 records/second\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:15.243] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 460, \"num_examples\": 17, \"num_bytes\": 1723868}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] # Finished training epoch 50 on 4348 examples from 17 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss (name: value) total: 6.7809051064883965\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss (name: value) kld: 0.09348064529545166\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss (name: value) recons: 6.687424463384292\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss (name: value) logppx: 6.7809051064883965\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] #quality_metric: host=algo-2, epoch=50, train total_loss <loss>=6.7809051064883965\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:15.304] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 149, \"duration\": 56, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss (name: value) total: 6.829020380973816\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss (name: value) kld: 0.09526065737009048\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss (name: value) recons: 6.733759760856628\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss (name: value) logppx: 6.829020380973816\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] #validation_score (50): 6.829020380973816\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] patience losses:[6.855812907218933, 6.860332608222961, 6.864179015159607, 6.861696481704712, 6.846298456192017, 6.847935438156128, 6.859824061393738, 6.842593550682068, 6.84159779548645, 6.835043430328369] min patience loss:6.835043430328369 current loss:6.829020380973816 absolute loss difference:0.006023049354553223\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Timing: train: 0.46s, val: 0.06s, epoch: 0.53s\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] #progress_metric: host=algo-2, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928994.7832952, \"EndTime\": 1701928995.3100474, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 49, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 217400.0, \"count\": 1, \"min\": 217400, \"max\": 217400}, \"Total Batches Seen\": {\"sum\": 850.0, \"count\": 1, \"min\": 850, \"max\": 850}, \"Max Records Seen Between Resets\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Max Batches Seen Between Resets\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Reset Count\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Number of Records Since Last Reset\": {\"sum\": 4348.0, \"count\": 1, \"min\": 4348, \"max\": 4348}, \"Number of Batches Since Last Reset\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}}}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] #throughput_metric: host=algo-2, train throughput=8251.707093175413 records/second\u001b[0m\n",
      "\u001b[35m[2023-12-07 06:03:15.370] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 152, \"duration\": 56, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss (name: value) total: 6.830654740333557\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss (name: value) kld: 0.09522407129406929\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss (name: value) recons: 6.735430717468262\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss (name: value) logppx: 6.830654740333557\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] #quality_metric: host=algo-2, epoch=50, validation total_loss <loss>=6.830654740333557\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Loss of server-side model: 6.830654740333557\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Best model based on early stopping at epoch 50. Best loss: 6.829020380973816\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Topics from epoch:final (num_topics:20) [wetc 0.29, tu 0.65]:\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.40, 0.30] wheel ford bike engine car transmission dealer mile clutch battery rear ride bmw automatic shift lock temperature oil seat tire\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.28, 0.54] connector active felt motherboard clutch isa stopped disease bus ide external lock apart wheel yeah bike installation scsi plug ride\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.44, 1.00] game hockey espn season playoff nhl team detroit baseball pitcher toronto league ranger montreal pt player pen flyer boston vancouver\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.27, 0.67] diet motor tire infection doctor cheaper temperature rear foot wiretap cryptography vitamin weight fuel orbit stick traffic engine lunar rocket\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.34, 0.47] tire bike dealer rear oil mhz controller air fuel wave model voltage amp battery wheel essentially paying road pressure clutch\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.19, 0.65] violence msg fed detector bus abortion batf food paying slot mhz burst cache slave afford speech simms capable eat serve\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.25, 0.81] infection planning cop requirement removed accident dog wall driver ready radar turbo crypto simms mhz alive loaded depends waiting disease\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.51, 1.00] jesus god scripture bible sin eternal faith christ christianity verse spirit doctrine atheist atheism heaven biblical christian passage morality belief\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.31, 0.26] dealer tire oil clutch bike rear riding engine automatic transmission lock road wheel nec wave ford jumper battery simms amp\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.21, 0.46] scsi connector ide isa slot motherboard battery drive adapter pin oil meg jumper slave objective careful bus speed ride simms\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.33, 0.37] temperature dealer bmw battery ford wheel motherboard stopped riding road transmission bike positive rear cpu ide car ride heat handle\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.20, 0.78] escrow wiretap cipher nsa encryption pgp rsa de launch encrypted institute lunar clipper privacy convention algorithm science key committee agency\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.33, 0.74] abortion moon risk cancer happening disease moral health familiar choice dog patient hospital msg blood rock vitamin nec happen serious\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.22, 0.76] escrow rsa billion spacecraft de wiretap lunar moon nsa energy nuclear encryption gov privacy nasa aside ball japanese protocol secret\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.26, 0.81] driver pro evolution kid abortion crash bat logic food remain card koresh screen batf playing surely familiar abuse perfectly extremely\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.22, 0.72] msg food infection doctor pro compound atf koresh pain treatment nec moral effect occurs continue sometimes politics cache six meg\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.21, 0.97] do port lib zip vga font tcp directory sys usr printer ftp unix workstation modem tar openwindows disk xterm controller\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.28, 1.00] armenian armenia serdar ottoman argic genocide turkish istanbul max turk azeri giz azerbaijan village muslim greece soviet azerbaijani turkey bhj\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.33, 0.34] amp tire car transmission engine clutch bike battery riding mile ride ide oil dealer automatic bus shift replaced rider scsi\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] [0.26, 0.41] battery tire slave cpu heat mhz jumper bmw ide connector pin lock slot drive probe recognize power bike clutch rear\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Serializing model to /opt/ml/model/model_algo-2\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Saved checkpoint to \"/tmp/tmpfht6wt_6/state-0001.params\"\u001b[0m\n",
      "\u001b[35m[12/07/2023 06:03:15 INFO 140221811656512] Test data is not provided.\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1701928956.3877907, \"EndTime\": 1701928995.4543736, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 8704.787969589233, \"count\": 1, \"min\": 8704.787969589233, \"max\": 8704.787969589233}, \"epochs\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"model.score.time\": {\"sum\": 3086.552619934082, \"count\": 51, \"min\": 42.987823486328125, \"max\": 93.56379508972168}, \"early_stop.time\": {\"sum\": 3190.659761428833, \"count\": 50, \"min\": 46.26750946044922, \"max\": 98.93321990966797}, \"update.time\": {\"sum\": 30180.63187599182, \"count\": 50, \"min\": 509.43541526794434, \"max\": 846.9116687774658}, \"finalize.time\": {\"sum\": 138.1988525390625, \"count\": 1, \"min\": 138.1988525390625, \"max\": 138.1988525390625}, \"model.serialize.time\": {\"sum\": 3.952503204345703, \"count\": 1, \"min\": 3.952503204345703, \"max\": 3.952503204345703}, \"setuptime\": {\"sum\": 53.40433120727539, \"count\": 1, \"min\": 53.40433120727539, \"max\": 53.40433120727539}, \"totaltime\": {\"sum\": 39144.75631713867, \"count\": 1, \"min\": 39144.75631713867, \"max\": 39144.75631713867}}}\u001b[0m\n",
      "\u001b[34m[2023-12-07 06:03:15.366] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 152, \"duration\": 54, \"num_examples\": 5, \"num_bytes\": 434680}\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] Finished scoring on 1024 examples from 4 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] Loss (name: value) total: 6.8306660652160645\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] Loss (name: value) kld: 0.0953138880431652\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] Loss (name: value) recons: 6.735352158546448\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] Loss (name: value) logppx: 6.8306660652160645\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] #quality_metric: host=algo-1, epoch=50, validation total_loss <loss>=6.8306660652160645\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] Loss of server-side model: 6.8306660652160645\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] Best model based on early stopping at epoch 50. Best loss: 6.8306660652160645\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] Topics from epoch:final (num_topics:20) [wetc 0.29, tu 0.63]:\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.40, 0.30] bike engine car ford wheel dealer transmission mile clutch battery rear ride automatic bmw lock shift oil temperature seat tire\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.29, 0.54] connector active clutch motherboard isa disease bus felt ide external scsi lock stopped bike wheel plug portable apart installation pain\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.44, 1.00] game hockey espn season playoff nhl team detroit baseball pitcher toronto league ranger pt montreal player pen flyer boston vancouver\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.26, 0.69] diet motor tire doctor infection temperature cheaper rear foot fuel engine wiretap weight traffic orbit vitamin rocket cryptography stick escrow\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.35, 0.41] tire bike dealer oil mhz rear controller air fuel model amp battery wave voltage drive rom wheel road clutch pressure\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.19, 0.64] violence msg bus fed abortion food detector batf mhz paying slot cache burst simms speech afford slave capable drive bank\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.26, 0.80] infection cop planning requirement dog removed driver accident ready wall simms turbo radar mhz crypto disease waiting depends basically loaded\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.51, 1.00] jesus god scripture bible sin faith eternal christ christianity verse atheist spirit doctrine atheism heaven christian biblical belief morality passage\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.34, 0.26] dealer tire oil clutch bike rear engine automatic riding lock road transmission wheel wave nec battery jumper car ford simms\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.24, 0.44] scsi connector ide isa slot motherboard drive battery adapter pin meg jumper oil speed bus mhz objective simms bike transfer\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.33, 0.35] temperature dealer bmw battery ford motherboard wheel bike car road transmission stopped rear riding cpu ide air positive heat ride\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.19, 0.76] escrow wiretap nsa encryption cipher pgp launch de lunar clipper rsa encrypted institute privacy key algorithm convention science solar agency\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.34, 0.70] abortion moon risk cancer disease moral health patient dog happening msg familiar choice hospital nec doctor blood happen pain rock\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.20, 0.73] escrow rsa billion spacecraft de wiretap nsa moon lunar encryption energy nuclear gov nasa privacy ball aside algorithm protocol drug\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.22, 0.76] driver evolution pro abortion kid card food logic screen koresh batf bat crash remain playing israel stupid familiar abuse atf\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.22, 0.62] msg food doctor infection pro compound atf koresh pain treatment nec moral effect cache continue sometimes six card politics meg\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.22, 0.97] do port lib zip vga font tcp directory sys printer usr ftp unix modem workstation tar disk openwindows controller file\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.28, 1.00] armenian armenia serdar ottoman argic max genocide turkish istanbul turk azeri giz azerbaijan village muslim soviet azerbaijani greece turkey bhj\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.33, 0.30] amp tire car engine transmission clutch bike battery mile ride riding ide oil dealer automatic bus scsi shift replaced mhz\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] [0.27, 0.38] battery tire cpu mhz slave heat ide pin jumper connector bmw drive lock power slot bike model clutch probe mile\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] Saved checkpoint to \"/tmp/tmpspcnfeql/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[12/07/2023 06:03:15 INFO 140009233762112] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701928955.8939745, \"EndTime\": 1701928995.4547918, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 9198.16541671753, \"count\": 1, \"min\": 9198.16541671753, \"max\": 9198.16541671753}, \"epochs\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"model.score.time\": {\"sum\": 2887.4218463897705, \"count\": 51, \"min\": 43.492794036865234, \"max\": 85.03484725952148}, \"early_stop.time\": {\"sum\": 3000.6728172302246, \"count\": 50, \"min\": 46.174049377441406, \"max\": 90.8358097076416}, \"update.time\": {\"sum\": 29506.418704986572, \"count\": 50, \"min\": 487.1346950531006, \"max\": 836.4453315734863}, \"finalize.time\": {\"sum\": 138.57793807983398, \"count\": 1, \"min\": 138.57793807983398, \"max\": 138.57793807983398}, \"model.serialize.time\": {\"sum\": 4.101037979125977, \"count\": 1, \"min\": 4.101037979125977, \"max\": 4.101037979125977}, \"setuptime\": {\"sum\": 56.96225166320801, \"count\": 1, \"min\": 56.96225166320801, \"max\": 56.96225166320801}, \"totaltime\": {\"sum\": 39648.60248565674, \"count\": 1, \"min\": 39648.60248565674, \"max\": 39648.60248565674}}}\u001b[0m\n",
      "\n",
      "2023-12-07 06:03:34 Uploading - Uploading generated training model\n",
      "2023-12-07 06:03:34 Completed - Training job completed\n",
      "Training seconds: 616\n",
      "Billable seconds: 616\n"
     ]
    }
   ],
   "source": [
    "# ntm.fit({'train': s3_train, 'validation': s3_train, 'auxiliary': s3_aux})\n",
    "ntm.fit({'train': s3_train, 'validation': s3_val, 'auxiliary': s3_aux})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the message\n",
    "\n",
    "> `===== Job Complete =====`\n",
    "\n",
    "at the bottom of the output logs, then training has successfully completed, and the output NTM model was stored in the specified output path.\n",
    "\n",
    "You can also view information about a training job on the SageMaker console. In the left navigation pane, under **Training**, choose **Training jobs**. Then, select the training job that matches the training job name from the following cell's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: ntm-2023-12-07-05-56-29-770\n"
     ]
    }
   ],
   "source": [
    "print('Training job name: {}'.format(ntm.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above that contains the log information for the training job, scroll until you find a line similar to the one in the following cell.\n",
    "\n",
    "**Tip:** Look for the phrase `Topics from epoch:final`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [05/04/2021 02:01:05 INFO 140593644394304] Topics from epoch:final (num_topics:20) [wetc 0.33, tu 0.68]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two numbers are of interest here: **wetc** and **tu**.\n",
    "\n",
    "- **wetc** is the *word embedding topic coherence* and indicates the degree of topic coherence. A higher number indicates a higher degree of topic coherence.\n",
    "- **tu** is the *topic uniqueness* metric and indicates how unique the terms are within the topic. A higher number indicates that the topic terms are more unique.\n",
    "\n",
    "In the example cell, the wetc is average at 0.33, and the tu is above average at 0.68."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the line that displays the overall wetc and tu metrics, you should see a list of topics that were identified along with the words that comprise that topic. Note that the topics are not named. That task still requires a human. For each topic, you see its wetc and tu scores, as well as the top words within that topic. \n",
    "\n",
    "Review these words, and try to determine a name for each topic.\n",
    "\n",
    "**Note:** Your results may be different than those in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [05/04/2021 02:01:05 INFO 140593644394304] [0.60, 0.80] game win playoff player team espn season play detroit baseball cup league score pitcher nhl goal toronto played hockey montreal\n",
    "\n",
    "Topic 0 seems to be about sports.\n",
    "\n",
    "    [05/04/2021 02:01:05 INFO 140593644394304] [0.57, 1.00] christ jesus god scripture doctrine christian sin bible faith christianity atheist church religion islam holy heaven biblical eternal morality belief\n",
    "\n",
    "Topic 1 seems to be about religion.\n",
    "\n",
    "    [05/04/2021 02:01:05 INFO 140593644394304] [0.30, 0.97] scsi ide motherboard controller drive mhz connector isa slot bus cpu jumper pin meg adapter floppy simms external cache speed\n",
    "    \n",
    "Topic 2 seems to be about computers.\n",
    "\n",
    "The following topic has a low uniqueness score. It's not clear what this topic would be. Is it something to do with motorcycles, food, or something else? You could name this '**unknown**'.\n",
    "\n",
    "    [05/04/2021 02:01:05 INFO 140593644394304] [0.35, 0.39] clutch eat bike doctor stopped msg riding watching pitch wheel ride hit food pain feeling personally fix sometimes rear tend\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Using the model for inference\n",
    "([Go to top](#Lab-6.2:-Implementing-Topic-Extraction-with-NTM))\n",
    "\n",
    "Now that you have a trained NTM model, use it to perform inference on data. For this example, that means predicting the topic mixture that represents a given document.\n",
    "\n",
    "To create an inference endpoint, use the SageMaker Python SDK `deploy()` function from the job that you defined previously. Specify the instance type where inference is computed as well as an initial number of instances to launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: ntm-2023-12-07-06-04-55-315\n",
      "INFO:sagemaker:Creating endpoint-config with name ntm-2023-12-07-06-04-55-315\n",
      "INFO:sagemaker:Creating endpoint with name ntm-2023-12-07-06-04-55-315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now have a functioning SageMaker NTM inference endpoint.\n",
    "\n",
    "You can confirm the endpoint configuration and status in the SageMaker console. In the left navigation pane, under **Inference**, choose **Endpoints**. Then, select the endpoint that matches the endpoint name from the following cell's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: ntm-2023-12-07-06-04-55-315\n"
     ]
    }
   ],
   "source": [
    "print('Endpoint name: {}'.format(ntm_predictor.endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data serialization and deserialization\n",
    "\n",
    "You can pass data in a variety of formats to the inference endpoint. First, you will pass CSV-formatted data. Use the SageMaker Python SDK utilities `csv_serializer` and `json_deserializer` to configure the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.content_types = 'text/csv'\n",
    "ntm_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "ntm_predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,  pass five examples from the test set to the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'topic_weights': [0.0514694117, 0.051962819, 0.0396383405, 0.0441860631, 0.0434924178, 0.0531198457, 0.0491438061, 0.1018232554, 0.046644222, 0.0536713153, 0.0487449057, 0.0358812995, 0.0575957447, 0.0415451452, 0.0619677678, 0.0661710426, 0.0154691683, 0.0472264886, 0.0442499369, 0.0459970348]}, {'topic_weights': [0.0318500325, 0.0376338623, 0.0231083725, 0.0369933397, 0.0372384414, 0.0397675671, 0.0416224711, 0.0448245816, 0.0343550891, 0.0375960544, 0.0357508548, 0.0422775783, 0.0378828011, 0.0418341532, 0.0408849642, 0.0426723249, 0.3053934276, 0.0186631717, 0.0334900692, 0.0361608677]}, {'topic_weights': [0.0348205045, 0.0403190479, 0.0245860275, 0.0442737788, 0.0404745117, 0.0440425426, 0.0476625264, 0.0759000182, 0.0370758846, 0.037191052, 0.038959939, 0.0567218326, 0.0439429395, 0.0522889085, 0.0482399017, 0.0450523347, 0.1960291564, 0.0225428883, 0.0340912789, 0.0357849225]}, {'topic_weights': [0.0347538739, 0.0419525281, 0.0232556872, 0.0394195281, 0.0412891805, 0.0586573966, 0.04755551, 0.0635185465, 0.0386626385, 0.0461379923, 0.0417659916, 0.0428510346, 0.0494807065, 0.0432292297, 0.062662892, 0.0587814897, 0.0267276559, 0.1574978828, 0.0382379964, 0.0435623005]}, {'topic_weights': [0.0252622142, 0.0256638471, 0.0185436364, 0.0245494451, 0.0273309443, 0.0306691658, 0.0282655098, 0.3909880519, 0.026821699, 0.0303125829, 0.0266691074, 0.0264764596, 0.0259964876, 0.0277833883, 0.0356626622, 0.0297704134, 0.0353830904, 0.1069369763, 0.0265388116, 0.0303754453]}]}\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array(test_vectors.todense())\n",
    "results = ntm_predictor.predict(test_data[:5])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output format of the SageMaker NTM inference endpoint is a Python dictionary with the following format.\n",
    "\n",
    "```\n",
    "{\n",
    "  'predictions': [\n",
    "    {'topic_weights': [ ... ] },\n",
    "    {'topic_weights': [ ... ] },\n",
    "    {'topic_weights': [ ... ] },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Extract the topic weights that correspond to each of the input documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05146941 0.05196282 0.03963834 0.04418606 0.04349242 0.05311985\n",
      "  0.04914381 0.10182326 0.04664422 0.05367132 0.04874491 0.0358813\n",
      "  0.05759574 0.04154515 0.06196777 0.06617104 0.01546917 0.04722649\n",
      "  0.04424994 0.04599703]\n",
      " [0.03185003 0.03763386 0.02310837 0.03699334 0.03723844 0.03976757\n",
      "  0.04162247 0.04482458 0.03435509 0.03759605 0.03575085 0.04227758\n",
      "  0.0378828  0.04183415 0.04088496 0.04267232 0.30539343 0.01866317\n",
      "  0.03349007 0.03616087]\n",
      " [0.0348205  0.04031905 0.02458603 0.04427378 0.04047451 0.04404254\n",
      "  0.04766253 0.07590002 0.03707588 0.03719105 0.03895994 0.05672183\n",
      "  0.04394294 0.05228891 0.0482399  0.04505233 0.19602916 0.02254289\n",
      "  0.03409128 0.03578492]\n",
      " [0.03475387 0.04195253 0.02325569 0.03941953 0.04128918 0.0586574\n",
      "  0.04755551 0.06351855 0.03866264 0.04613799 0.04176599 0.04285103\n",
      "  0.04948071 0.04322923 0.06266289 0.05878149 0.02672766 0.15749788\n",
      "  0.038238   0.0435623 ]\n",
      " [0.02526221 0.02566385 0.01854364 0.02454945 0.02733094 0.03066917\n",
      "  0.02826551 0.39098805 0.0268217  0.03031258 0.02666911 0.02647646\n",
      "  0.02599649 0.02778339 0.03566266 0.02977041 0.03538309 0.10693698\n",
      "  0.02653881 0.03037545]]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the topic names in the following cell with the topic names that you determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.font_manager:Matplotlib is building the font cache; this may take a moment.\n",
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "colnames = pd.DataFrame({'topics':['topic 0', 'topic 1', 'topic 2', 'topic 3', 'topic 4', 'topic 5', 'topic 6','topic 7','topic 8','topic 9',\n",
    "       'topic 10', 'topic 11', 'topic 12', 'topic 13', 'topic 14', 'topic 15', 'topic 16','topic 17','topic 18','topic 19']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use a bar plot to take a look at how the 20 topics are assigned to the 5 test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Topic ID')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fs = 12\n",
    "df=pd.DataFrame(predictions.T)\n",
    "df.index = colnames['topics']\n",
    "df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "plt.xlabel('Topic ID', fontsize=fs+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could improve the model by adding or removing specific words to influence topics, increasing or decreasing the number of topics, and trying different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the endpoint\n",
    "\n",
    "Finally, delete the endpoint before you close the notebook.\n",
    "\n",
    "To restart the endpoint, you can follow the code in section 5 using the same `endpoint_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: ntm-2023-12-07-06-04-55-315\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(ntm_predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Exploring the model\n",
    "([Go to top](#Lab-6.2:-Implementing-Topic-Extraction-with-NTM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: This section provides a deeper exploration of the trained models. The demonstrated functionalities may not be fully supported or guaranteed. For example, the parameter names may change without notice.**\n",
    "\n",
    "The trained model artifact is a compressed package of MXNet models from the two workers. To explore the model, you first need to install MXNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mxnet) (1.26.1)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mxnet) (2.31.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/mxnet/numpy/utils.py:37: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  bool = onp.bool\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# If you use the conda_mxnet_p36 kernel, MXNet is already installed; otherwise, uncomment the following line to install it.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install mxnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmxnet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmx\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/mxnet/__init__.py:33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# version info\u001b[39;00m\n\u001b[1;32m     31\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39m__version__\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m contrib\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray \u001b[38;5;28;01mas\u001b[39;00m nd\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/mxnet/contrib/__init__.py:30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograd\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorboard\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m onnx\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/mxnet/contrib/text/__init__.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocab\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embedding\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/mxnet/contrib/text/embedding.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_np_array\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m _mx_np\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numpy_extension \u001b[38;5;28;01mas\u001b[39;00m _mx_npx\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister\u001b[39m(embedding_cls):\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/mxnet/numpy/__init__.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _op\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _register\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/mxnet/numpy/multiarray.py:47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _internal \u001b[38;5;28;01mas\u001b[39;00m _npi\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _storage_type, from_numpy\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_np_op\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import,unused-wildcard-import\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fallback\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/mxnet/numpy/utils.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m int64 \u001b[38;5;241m=\u001b[39m onp\u001b[38;5;241m.\u001b[39mint64\n\u001b[1;32m     36\u001b[0m bool_ \u001b[38;5;241m=\u001b[39m onp\u001b[38;5;241m.\u001b[39mbool_\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43monp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\n\u001b[1;32m     39\u001b[0m pi \u001b[38;5;241m=\u001b[39m onp\u001b[38;5;241m.\u001b[39mpi\n\u001b[1;32m     40\u001b[0m inf \u001b[38;5;241m=\u001b[39m onp\u001b[38;5;241m.\u001b[39minf\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/numpy/__init__.py:324\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# If you use the conda_mxnet_p36 kernel, MXNet is already installed; otherwise, uncomment the following line to install it.\n",
    "!pip install mxnet \n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unpack the artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20newsgroups-ntm/output/ntm-2023-12-07-05-56-29-770/output/model.tar.gz'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join(output_prefix, ntm._current_job_name, 'output/model.tar.gz')\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.resource('s3').Bucket(bucket).download_file(model_path, 'downloaded_model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "model_algo-2\n",
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n",
      "model_algo-1\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf 'downloaded_model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  model_algo-2\n",
      " extracting: meta.json               \n",
      " extracting: symbol.json             \n",
      " extracting: params                  \n"
     ]
    }
   ],
   "source": [
    "# Use flag -o to overwrite the previously unzipped content\n",
    "!unzip -o model_algo-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model parameters, and extract the weight matrix $W$ in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmx\u001b[49m\u001b[38;5;241m.\u001b[39mndarray\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m W \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marg:projection_weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mx' is not defined"
     ]
    }
   ],
   "source": [
    "model = mx.ndarray.load('params')\n",
    "\n",
    "W = model['arg:projection_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize each topic as a word cloud. The size of each word is proportional to the pseudo-probability of the word appearing under each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from wordcloud) (1.26.1)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from wordcloud) (10.1.0)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from wordcloud) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (455 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m455.4/455.4 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "import wordcloud as wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "word_to_id = dict()\n",
    "for i, v in enumerate(vocab_list):\n",
    "    word_to_id[v] = i\n",
    "\n",
    "limit = 24\n",
    "n_col = 4\n",
    "counter = 0\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for ind in range(num_topics):\n",
    "\n",
    "    if counter >= limit:\n",
    "        break\n",
    "\n",
    "    title_str = 'Topic{}'.format(ind)\n",
    "\n",
    "    #pvals = mx.nd.softmax(W[:, ind]).asnumpy()\n",
    "    pvals = mx.nd.softmax(mx.nd.array(W[:, ind])).asnumpy()\n",
    "\n",
    "    word_freq = dict()\n",
    "    for k in word_to_id.keys():\n",
    "        i = word_to_id[k]\n",
    "        word_freq[k] =pvals[i]\n",
    "\n",
    "    wordcloud = wc.WordCloud(background_color='white').fit_words(word_freq)\n",
    "\n",
    "    plt.subplot(limit // n_col, n_col, counter+1)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title_str)\n",
    "    #plt.close()\n",
    "\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have completed this lab, and you can now end the lab by following the lab guide instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2023 Amazon Web Services, Inc. or its affiliates. All rights reserved. This work may not be reproduced or redistributed, in whole or in part, without prior written permission from Amazon Web Services, Inc. Commercial copying, lending, or selling is prohibited. All trademarks are the property of their owners.*\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "vscode": {
   "interpreter": {
    "hash": "b71a13339a0be9489ff337af97259fe0ed71e682663adc836bae31ac651d564e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
